{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnackTrack ML --- RNN/GRU Training for Meal Sequence Prediction\n",
    "\n",
    "This notebook trains a **GRU-based Recurrent Neural Network** to predict a user's next meal based on\n",
    "their chronological eating history. The RNN captures temporal patterns such as:\n",
    "\n",
    "- **Time-of-day preferences** (e.g., lighter breakfasts, hearty dinners)\n",
    "- **Weekly meal rotation** (e.g., Taco Tuesday, Fish Friday)\n",
    "- **Seasonal and progressive dietary changes** over time\n",
    "\n",
    "### Architecture\n",
    "\n",
    "| Component | Dimension | Description |\n",
    "|-----------|-----------|-------------|\n",
    "| Input | 39D | 32D recipe embedding + 7D cyclical time features |\n",
    "| GRU Hidden | 64D | Custom GRU cell matching the NumPy production code |\n",
    "| Output | 32D | Predicted next-meal recipe embedding |\n",
    "\n",
    "### Why a custom GRU cell?\n",
    "\n",
    "PyTorch's built-in `nn.GRUCell` applies the reset gate **before** the hidden-to-hidden\n",
    "multiplication: `h_candidate = tanh(W_h @ x + U_h @ (r * h_prev) + b_h)`. Our production\n",
    "NumPy code applies it **after**: `h_candidate = tanh(W_h @ x + (r * h_prev) @ U_h + b_h)`.\n",
    "This is a subtle but critical difference --- if we used `nn.GRUCell`, the exported weights\n",
    "would produce different outputs in production. We therefore implement a custom GRU cell\n",
    "whose forward pass is an exact PyTorch translation of the NumPy `gru_step()`.\n",
    "\n",
    "### Critical note\n",
    "\n",
    "The production RNN (`app/recommender/rnn.py`) currently uses **random placeholder weights**.\n",
    "This notebook trains real weights from data and exports them in the exact format expected\n",
    "by `utils/weight_io.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Path setup: make notebook utils importable\n",
    "# ---------------------------------------------------------------------------\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from notebooks.utils.plot_helpers import setup_plot_style, plot_loss_curves, SNACKTRACK_COLORS, PALETTE\n",
    "from notebooks.utils.data_loader import load_kaggle_dataset, _encode_time_features\n",
    "from notebooks.utils.weight_io import save_rnn_weights, load_rnn_weights, RNN_WEIGHT_SHAPES\n",
    "\n",
    "setup_plot_style()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch {torch.__version__} | Device: {DEVICE}\")\n",
    "print(f\"NumPy {np.__version__} | Pandas {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Sequence Data\n",
    "\n",
    "We build user meal sequences from two complementary data sources:\n",
    "\n",
    "1. **Food.com interactions** (`foodcom_interactions`) --- large-scale user-recipe interaction\n",
    "   logs with timestamps, providing real chronological meal sequences.\n",
    "2. **Daily Food Nutrition** (`daily_food_nutrition`) --- daily food logs with meal-level\n",
    "   detail including meal type and nutritional information.\n",
    "\n",
    "For each meal event we construct a **39-dimensional input vector**:\n",
    "- **32D recipe embedding**: from the recipe's ingredient/nutrition vector, or a\n",
    "  feature-based proxy when the full vector is unavailable.\n",
    "- **7D time features**: cyclical encoding of day-of-week (sin/cos), hour (sin/cos),\n",
    "  meal type (sin/cos), and a normalized meal-type index.\n",
    "\n",
    "The time encoding uses sin/cos pairs so that the model understands that\n",
    "Sunday (6) and Monday (0) are adjacent, and 23:00 is close to 00:00.\n",
    "\n",
    "We then create **sliding windows of length 20**, where the target for each\n",
    "window is the 32D recipe embedding of the *next* meal. The train/validation\n",
    "split is done **by user** (not by sequence) to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "SEQ_LEN = 20           # sliding window length\n",
    "RECIPE_EMB_DIM = 32    # recipe embedding dimensionality\n",
    "TIME_FEAT_DIM = 7      # cyclical time features\n",
    "INPUT_DIM = RECIPE_EMB_DIM + TIME_FEAT_DIM  # 39\n",
    "VAL_FRACTION = 0.2     # fraction of users held out for validation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Time feature encoding (matches production MealSequenceRNN.encode_time_features)\n",
    "# ---------------------------------------------------------------------------\n",
    "def encode_time_features(logged_at, meal_type):\n",
    "    \"\"\"Encode temporal context into a 7D vector with cyclical sin/cos pairs.\n",
    "\n",
    "    This MUST match the production code in app/recommender/rnn.py exactly.\n",
    "    \"\"\"\n",
    "    dow = logged_at.weekday() if hasattr(logged_at, \"weekday\") else 0\n",
    "    hour = logged_at.hour if hasattr(logged_at, \"hour\") else 12\n",
    "\n",
    "    day_sin = np.sin(2 * np.pi * dow / 7)\n",
    "    day_cos = np.cos(2 * np.pi * dow / 7)\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    meal_types = {\"breakfast\": 0, \"lunch\": 1, \"dinner\": 2, \"snack\": 3}\n",
    "    meal_idx = meal_types.get(meal_type, 1)\n",
    "    meal_sin = np.sin(2 * np.pi * meal_idx / 4)\n",
    "    meal_cos = np.cos(2 * np.pi * meal_idx / 4)\n",
    "\n",
    "    return np.array([day_sin, day_cos, hour_sin, hour_cos,\n",
    "                     meal_sin, meal_cos, meal_idx / 3.0])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Recipe embedding helper\n",
    "# ---------------------------------------------------------------------------\n",
    "def make_recipe_embedding(row, nutrition_cols=None):\n",
    "    \"\"\"Create a 32D recipe embedding from available nutritional features.\n",
    "\n",
    "    Uses a normalised feature-based proxy: the first few dimensions encode\n",
    "    key macro-nutrients, and remaining dimensions are zero-padded.\n",
    "    \"\"\"\n",
    "    emb = np.zeros(RECIPE_EMB_DIM, dtype=np.float64)\n",
    "    emb[0] = (row.get(\"calories\") or 0) / 1000.0\n",
    "    emb[1] = (row.get(\"protein_g\") or row.get(\"protein\") or 0) / 100.0\n",
    "    emb[2] = (row.get(\"carbs_g\") or row.get(\"carbohydrate\") or row.get(\"carbs\") or 0) / 200.0\n",
    "    emb[3] = (row.get(\"fat_g\") or row.get(\"total_fat\") or row.get(\"fat\") or 0) / 100.0\n",
    "    emb[4] = (row.get(\"sodium_mg\") or row.get(\"sodium\") or 0) / 2300.0\n",
    "    emb[5] = (row.get(\"fiber_g\") or row.get(\"fiber\") or 0) / 30.0\n",
    "    emb[6] = (row.get(\"sugar_g\") or row.get(\"sugar\") or 0) / 50.0\n",
    "    return emb\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Load datasets\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "sequences_all = []   # list of (user_id, [(emb_39d, target_32d), ...])\n",
    "user_sequence_map = {}  # user_id -> list of (input_window, target)\n",
    "\n",
    "# --- Source 1: Food.com interactions ---\n",
    "try:\n",
    "    interactions_df = load_kaggle_dataset(\"foodcom_interactions\")\n",
    "    print(f\"  Food.com interactions: {len(interactions_df):,} rows\")\n",
    "\n",
    "    # Parse date column\n",
    "    date_col = None\n",
    "    for col in [\"date\", \"submitted\", \"created_at\", \"interaction_date\"]:\n",
    "        if col in interactions_df.columns:\n",
    "            date_col = col\n",
    "            break\n",
    "    if date_col is None:\n",
    "        # Try to find any date-like column\n",
    "        for col in interactions_df.columns:\n",
    "            if \"date\" in col.lower() or \"time\" in col.lower():\n",
    "                date_col = col\n",
    "                break\n",
    "\n",
    "    if date_col:\n",
    "        interactions_df[\"timestamp\"] = pd.to_datetime(interactions_df[date_col], errors=\"coerce\")\n",
    "    else:\n",
    "        # Assign synthetic timestamps so we can still train\n",
    "        interactions_df[\"timestamp\"] = pd.date_range(\n",
    "            start=\"2020-01-01\", periods=len(interactions_df), freq=\"h\"\n",
    "        )\n",
    "    interactions_df = interactions_df.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    # Identify user and recipe columns\n",
    "    user_col = next((c for c in [\"user_id\", \"author_id\", \"contributor_id\"] if c in interactions_df.columns), None)\n",
    "    recipe_col = next((c for c in [\"recipe_id\", \"id\"] if c in interactions_df.columns), None)\n",
    "\n",
    "    if user_col and recipe_col:\n",
    "        # Identify available nutrition columns\n",
    "        nutrition_present = [c for c in [\"calories\", \"protein\", \"protein_g\",\n",
    "                                         \"carbohydrate\", \"carbs_g\", \"total_fat\",\n",
    "                                         \"fat_g\", \"sodium\", \"sodium_mg\",\n",
    "                                         \"fiber\", \"fiber_g\", \"sugar\", \"sugar_g\"]\n",
    "                            if c in interactions_df.columns]\n",
    "\n",
    "        # Infer meal type from hour\n",
    "        def infer_meal_type(hour):\n",
    "            if 5 <= hour < 11:\n",
    "                return \"breakfast\"\n",
    "            elif 11 <= hour < 15:\n",
    "                return \"lunch\"\n",
    "            elif 15 <= hour < 21:\n",
    "                return \"dinner\"\n",
    "            return \"snack\"\n",
    "\n",
    "        # Group by user and build sequences\n",
    "        user_groups = interactions_df.sort_values(\"timestamp\").groupby(user_col)\n",
    "\n",
    "        for uid, group in tqdm(user_groups, desc=\"Building Food.com sequences\", leave=False):\n",
    "            if len(group) < SEQ_LEN + 1:\n",
    "                continue\n",
    "\n",
    "            meal_vectors = []\n",
    "            for _, row in group.iterrows():\n",
    "                emb = make_recipe_embedding(row)\n",
    "                ts = row[\"timestamp\"]\n",
    "                mt = infer_meal_type(ts.hour if hasattr(ts, \"hour\") else 12)\n",
    "                time_feat = encode_time_features(ts, mt)\n",
    "                meal_vectors.append(np.concatenate([emb, time_feat]))\n",
    "\n",
    "            # Sliding windows\n",
    "            windows = []\n",
    "            for i in range(len(meal_vectors) - SEQ_LEN):\n",
    "                inp = np.array(meal_vectors[i : i + SEQ_LEN])  # (20, 39)\n",
    "                target = meal_vectors[i + SEQ_LEN][:RECIPE_EMB_DIM]  # (32,)\n",
    "                windows.append((inp, target))\n",
    "\n",
    "            if windows:\n",
    "                user_key = f\"foodcom_{uid}\"\n",
    "                user_sequence_map[user_key] = windows\n",
    "\n",
    "    print(f\"  -> {len(user_sequence_map)} users with enough history from Food.com\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"  Food.com interactions not found. Skipping.\")\n",
    "\n",
    "# --- Source 2: Daily Food Nutrition ---\n",
    "try:\n",
    "    nutrition_df = load_kaggle_dataset(\"daily_food_nutrition\")\n",
    "    print(f\"  Daily Food Nutrition: {len(nutrition_df):,} rows\")\n",
    "\n",
    "    # Parse date/time\n",
    "    date_col = None\n",
    "    for col in [\"date\", \"log_date\", \"day\", \"timestamp\"]:\n",
    "        if col in nutrition_df.columns:\n",
    "            date_col = col\n",
    "            break\n",
    "\n",
    "    if date_col:\n",
    "        nutrition_df[\"timestamp\"] = pd.to_datetime(nutrition_df[date_col], errors=\"coerce\")\n",
    "    else:\n",
    "        nutrition_df[\"timestamp\"] = pd.date_range(\n",
    "            start=\"2021-01-01\", periods=len(nutrition_df), freq=\"h\"\n",
    "        )\n",
    "    nutrition_df = nutrition_df.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    # Identify columns\n",
    "    user_col_n = next((c for c in [\"user_id\", \"user\", \"person\", \"name\"]\n",
    "                       if c in nutrition_df.columns), None)\n",
    "    meal_col = next((c for c in [\"meal_type\", \"meal\", \"category\"]\n",
    "                     if c in nutrition_df.columns), None)\n",
    "\n",
    "    if user_col_n is None:\n",
    "        # Create synthetic user IDs from row index groups\n",
    "        nutrition_df[\"user_id_synth\"] = nutrition_df.index // 100\n",
    "        user_col_n = \"user_id_synth\"\n",
    "\n",
    "    user_groups_n = nutrition_df.sort_values(\"timestamp\").groupby(user_col_n)\n",
    "\n",
    "    n_before = len(user_sequence_map)\n",
    "    for uid, group in tqdm(user_groups_n, desc=\"Building nutrition sequences\", leave=False):\n",
    "        if len(group) < SEQ_LEN + 1:\n",
    "            continue\n",
    "\n",
    "        meal_vectors = []\n",
    "        for _, row in group.iterrows():\n",
    "            emb = make_recipe_embedding(row)\n",
    "            ts = row[\"timestamp\"]\n",
    "            mt = row[meal_col].lower() if meal_col and pd.notna(row.get(meal_col)) else \"lunch\"\n",
    "            time_feat = encode_time_features(ts, mt)\n",
    "            meal_vectors.append(np.concatenate([emb, time_feat]))\n",
    "\n",
    "        windows = []\n",
    "        for i in range(len(meal_vectors) - SEQ_LEN):\n",
    "            inp = np.array(meal_vectors[i : i + SEQ_LEN])\n",
    "            target = meal_vectors[i + SEQ_LEN][:RECIPE_EMB_DIM]\n",
    "            windows.append((inp, target))\n",
    "\n",
    "        if windows:\n",
    "            user_key = f\"nutrition_{uid}\"\n",
    "            user_sequence_map[user_key] = windows\n",
    "\n",
    "    print(f\"  -> {len(user_sequence_map) - n_before} additional users from Daily Food Nutrition\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"  Daily Food Nutrition not found. Skipping.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Fallback: generate synthetic sequences for development/testing\n",
    "# ---------------------------------------------------------------------------\n",
    "if len(user_sequence_map) == 0:\n",
    "    print(\"\\n  WARNING: No real datasets available. Generating synthetic sequences.\")\n",
    "    print(\"  Run notebook 00 first to download the datasets for real training.\")\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    N_SYNTH_USERS = 200\n",
    "    MEALS_PER_USER = 60\n",
    "\n",
    "    for u in range(N_SYNTH_USERS):\n",
    "        # Each user has a latent \"taste\" vector that slowly drifts\n",
    "        taste = rng.standard_normal(RECIPE_EMB_DIM) * 0.3\n",
    "        meal_vectors = []\n",
    "        base_date = pd.Timestamp(\"2023-01-01\")\n",
    "\n",
    "        for m in range(MEALS_PER_USER):\n",
    "            # Drift taste slightly\n",
    "            taste += rng.standard_normal(RECIPE_EMB_DIM) * 0.01\n",
    "            emb = taste + rng.standard_normal(RECIPE_EMB_DIM) * 0.1\n",
    "\n",
    "            ts = base_date + pd.Timedelta(hours=m * 6)  # ~4 meals/day\n",
    "            hour = ts.hour\n",
    "            if 5 <= hour < 11:\n",
    "                mt = \"breakfast\"\n",
    "            elif 11 <= hour < 15:\n",
    "                mt = \"lunch\"\n",
    "            elif 15 <= hour < 21:\n",
    "                mt = \"dinner\"\n",
    "            else:\n",
    "                mt = \"snack\"\n",
    "\n",
    "            time_feat = encode_time_features(ts, mt)\n",
    "            meal_vectors.append(np.concatenate([emb, time_feat]))\n",
    "\n",
    "        windows = []\n",
    "        for i in range(len(meal_vectors) - SEQ_LEN):\n",
    "            inp = np.array(meal_vectors[i : i + SEQ_LEN])\n",
    "            target = meal_vectors[i + SEQ_LEN][:RECIPE_EMB_DIM]\n",
    "            windows.append((inp, target))\n",
    "\n",
    "        user_sequence_map[f\"synth_{u}\"] = windows\n",
    "\n",
    "    print(f\"  Generated {N_SYNTH_USERS} synthetic users.\")\n",
    "\n",
    "print(f\"\\nTotal users with sequences: {len(user_sequence_map)}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Train/val split by user\n",
    "# ---------------------------------------------------------------------------\n",
    "all_users = sorted(user_sequence_map.keys())\n",
    "np.random.shuffle(all_users)\n",
    "split_idx = int(len(all_users) * (1 - VAL_FRACTION))\n",
    "train_users = set(all_users[:split_idx])\n",
    "val_users = set(all_users[split_idx:])\n",
    "\n",
    "train_inputs, train_targets = [], []\n",
    "val_inputs, val_targets = [], []\n",
    "\n",
    "for user, windows in user_sequence_map.items():\n",
    "    for inp, target in windows:\n",
    "        if user in train_users:\n",
    "            train_inputs.append(inp)\n",
    "            train_targets.append(target)\n",
    "        else:\n",
    "            val_inputs.append(inp)\n",
    "            val_targets.append(target)\n",
    "\n",
    "X_train = np.array(train_inputs, dtype=np.float32)\n",
    "y_train = np.array(train_targets, dtype=np.float32)\n",
    "X_val = np.array(val_inputs, dtype=np.float32)\n",
    "y_val = np.array(val_targets, dtype=np.float32)\n",
    "\n",
    "print(f\"Training:   {X_train.shape[0]:,} sequences from {len(train_users)} users\")\n",
    "print(f\"Validation: {X_val.shape[0]:,} sequences from {len(val_users)} users\")\n",
    "print(f\"Input shape:  {X_train.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Create PyTorch DataLoaders\n",
    "# ---------------------------------------------------------------------------\n",
    "class MealSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = MealSequenceDataset(X_train, y_train)\n",
    "val_dataset = MealSequenceDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)} | Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Sequence statistics\n",
    "# ---------------------------------------------------------------------------\n",
    "seq_counts = {u: len(w) for u, w in user_sequence_map.items()}\n",
    "total_sequences = sum(seq_counts.values())\n",
    "unique_users = len(seq_counts)\n",
    "avg_seq_per_user = np.mean(list(seq_counts.values()))\n",
    "median_seq_per_user = np.median(list(seq_counts.values()))\n",
    "max_seq_per_user = max(seq_counts.values())\n",
    "min_seq_per_user = min(seq_counts.values())\n",
    "\n",
    "print(f\"Sequence Statistics\")\n",
    "print(f\"{'=' * 45}\")\n",
    "print(f\"  Total sequences:           {total_sequences:,}\")\n",
    "print(f\"  Unique users:              {unique_users:,}\")\n",
    "print(f\"  Avg sequences per user:    {avg_seq_per_user:.1f}\")\n",
    "print(f\"  Median sequences per user: {median_seq_per_user:.1f}\")\n",
    "print(f\"  Min / Max per user:        {min_seq_per_user} / {max_seq_per_user}\")\n",
    "print(f\"  Train users / Val users:   {len(train_users)} / {len(val_users)}\")\n",
    "print(f\"  Sequence length (window):  {SEQ_LEN}\")\n",
    "print(f\"  Input dim:                 {INPUT_DIM}\")\n",
    "print(f\"  Target dim:                {RECIPE_EMB_DIM}\")\n",
    "\n",
    "# Distribution plot\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(list(seq_counts.values()), bins=50, color=SNACKTRACK_COLORS[\"primary\"], alpha=0.8,\n",
    "        edgecolor=\"white\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Sequences per User\")\n",
    "ax.set_ylabel(\"Number of Users\")\n",
    "ax.set_title(\"Distribution of Sequence Count per User\")\n",
    "ax.axvline(avg_seq_per_user, color=SNACKTRACK_COLORS[\"secondary\"], linestyle=\"--\",\n",
    "           label=f\"Mean = {avg_seq_per_user:.1f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Custom GRU Model\n",
    "\n",
    "We implement a custom `CustomGRUCell` whose `forward()` method is a line-by-line\n",
    "PyTorch translation of the NumPy `MealSequenceRNN.gru_step()`:\n",
    "\n",
    "```\n",
    "z = sigmoid(x @ Wz + h_prev @ Uz + bz)           # update gate\n",
    "r = sigmoid(x @ Wr + h_prev @ Ur + br)            # reset gate\n",
    "h_candidate = tanh(x @ Wh + (r * h_prev) @ Uh + bh)  # candidate\n",
    "h_new = (1 - z) * h_prev + z * h_candidate         # new hidden state\n",
    "```\n",
    "\n",
    "The weight matrices are defined as `nn.Parameter` with the **same shapes** as\n",
    "production (`Wz/Wr/Wh: (39, 64)`, `Uz/Ur/Uh: (64, 64)`, `bz/br/bh: (64,)`).\n",
    "After training, the weights can be exported directly without transposition\n",
    "(except for the output projection `nn.Linear`, which stores `weight` as\n",
    "`(out_features, in_features)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRUCell(nn.Module):\n",
    "    \"\"\"Custom GRU cell that exactly matches the NumPy production implementation.\n",
    "\n",
    "    Key difference from nn.GRUCell:\n",
    "      - nn.GRUCell computes: h_candidate = tanh(Wh @ x + Uh @ (r * h_prev) + bh)\n",
    "      - Our code computes:   h_candidate = tanh(x @ Wh + (r * h_prev) @ Uh + bh)\n",
    "\n",
    "    The reset gate is applied element-wise to h_prev BEFORE the matrix multiply\n",
    "    with Uh, which is mathematically equivalent to: (r * h_prev) @ Uh.\n",
    "    This differs from the standard formulation where r gates the result of Uh @ h_prev.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Update gate parameters\n",
    "        self.Wz = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
    "        self.Uz = nn.Parameter(torch.randn(hidden_dim, hidden_dim) * 0.1)\n",
    "        self.bz = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Reset gate parameters\n",
    "        self.Wr = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
    "        self.Ur = nn.Parameter(torch.randn(hidden_dim, hidden_dim) * 0.1)\n",
    "        self.br = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Candidate hidden state parameters\n",
    "        self.Wh = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
    "        self.Uh = nn.Parameter(torch.randn(hidden_dim, hidden_dim) * 0.1)\n",
    "        self.bh = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Single GRU step: exactly mirrors NumPy gru_step().\n",
    "\n",
    "        Args:\n",
    "            x: input at current timestep, shape (batch, input_dim)\n",
    "            h_prev: previous hidden state, shape (batch, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            h_new: updated hidden state, shape (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        z = torch.sigmoid(x @ self.Wz + h_prev @ self.Uz + self.bz)\n",
    "        r = torch.sigmoid(x @ self.Wr + h_prev @ self.Ur + self.br)\n",
    "        h_candidate = torch.tanh(x @ self.Wh + (r * h_prev) @ self.Uh + self.bh)\n",
    "        h_new = (1 - z) * h_prev + z * h_candidate\n",
    "        return h_new\n",
    "\n",
    "\n",
    "class MealSequenceModel(nn.Module):\n",
    "    \"\"\"Full model: CustomGRU unrolled over the sequence + linear output projection.\"\"\"\n",
    "\n",
    "    INPUT_DIM = 39\n",
    "    HIDDEN_DIM = 64\n",
    "    OUTPUT_DIM = 32\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gru_cell = CustomGRUCell(self.INPUT_DIM, self.HIDDEN_DIM)\n",
    "        self.output_proj = nn.Linear(self.HIDDEN_DIM, self.OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\"Process a batch of sequences and predict next-meal embedding.\n",
    "\n",
    "        Args:\n",
    "            sequence: (batch_size, seq_len, 39)\n",
    "\n",
    "        Returns:\n",
    "            predicted embedding: (batch_size, 32)\n",
    "        \"\"\"\n",
    "        batch_size = sequence.size(0)\n",
    "        h = torch.zeros(batch_size, self.HIDDEN_DIM, device=sequence.device)\n",
    "\n",
    "        for t in range(sequence.size(1)):\n",
    "            h = self.gru_cell(sequence[:, t, :], h)\n",
    "\n",
    "        return self.output_proj(h)\n",
    "\n",
    "\n",
    "# Instantiate\n",
    "model = MealSequenceModel().to(DEVICE)\n",
    "\n",
    "# Print parameter summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Architecture\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Parameter':<25} {'Shape':<20} {'Count':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:<23} {str(tuple(param.shape)):<20} {param.numel():>10,}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"  {'Total':<23} {'':<20} {total_params:>10,}\")\n",
    "print(f\"  {'Trainable':<23} {'':<20} {trainable_params:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop\n",
    "\n",
    "Training details:\n",
    "\n",
    "| Hyperparameter | Value | Rationale |\n",
    "|---------------|-------|-----------|\n",
    "| Loss | MSE | Regression on continuous recipe embeddings |\n",
    "| Optimizer | Adam | Adaptive LR, good for RNNs |\n",
    "| Learning rate | 1e-3 | Standard starting point for Adam |\n",
    "| Gradient clipping | max_norm=1.0 | Prevents exploding gradients in long sequences |\n",
    "| Epochs | 150 max | Upper bound; early stopping usually triggers earlier |\n",
    "| Early stopping | patience=15 | Stop if val loss hasn't improved in 15 epochs |\n",
    "| LR scheduler | ReduceLROnPlateau(patience=10) | Halve LR when val loss stagnates |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Training configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "NUM_EPOCHS = 150\n",
    "LEARNING_RATE = 1e-3\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "SCHEDULER_PATIENCE = 10\n",
    "SCHEDULER_FACTOR = 0.5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=SCHEDULER_PATIENCE,\n",
    "    factor=SCHEDULER_FACTOR, verbose=False\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Training loop\n",
    "# ---------------------------------------------------------------------------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "lr_history = []\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "best_state = None\n",
    "\n",
    "print(f\"Training for up to {NUM_EPOCHS} epochs...\")\n",
    "print(f\"  LR={LEARNING_RATE}, GradClip={GRAD_CLIP_NORM}, \"\n",
    "      f\"EarlyStop={EARLY_STOP_PATIENCE}, SchedulerPatience={SCHEDULER_PATIENCE}\")\n",
    "print()\n",
    "\n",
    "pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    n_train_batches = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        n_train_batches += 1\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / max(n_train_batches, 1)\n",
    "\n",
    "    # --- Validate ---\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    n_val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "\n",
    "            epoch_val_loss += loss.item()\n",
    "            n_val_batches += 1\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / max(n_val_batches, 1)\n",
    "\n",
    "    # --- LR scheduler ---\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # --- Bookkeeping ---\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    lr_history.append(current_lr)\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"train\": f\"{avg_train_loss:.6f}\",\n",
    "        \"val\": f\"{avg_val_loss:.6f}\",\n",
    "        \"lr\": f\"{current_lr:.1e}\",\n",
    "        \"best\": f\"{best_val_loss:.6f}\",\n",
    "        \"pat\": f\"{patience_counter}/{EARLY_STOP_PATIENCE}\",\n",
    "    })\n",
    "\n",
    "    if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}. \"\n",
    "              f\"Best val loss: {best_val_loss:.6f} at epoch {best_epoch}.\")\n",
    "        break\n",
    "\n",
    "# Restore best weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "print(f\"\\nTraining complete.\")\n",
    "print(f\"  Best epoch:    {best_epoch}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.6f}\")\n",
    "print(f\"  Final LR:      {lr_history[-1]:.1e}\")\n",
    "print(f\"  Total epochs:  {len(train_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Visualization\n",
    "\n",
    "We plot two charts:\n",
    "1. **Loss curves**: train vs validation loss over epochs, with the best epoch marked.\n",
    "2. **Learning rate schedule**: shows when the `ReduceLROnPlateau` scheduler reduced the LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# --- Loss curves ---\n",
    "ax = axes[0]\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "ax.plot(epochs_range, train_losses, label=\"Train Loss\",\n",
    "        color=SNACKTRACK_COLORS[\"primary\"], linewidth=2)\n",
    "ax.plot(epochs_range, val_losses, label=\"Val Loss\",\n",
    "        color=SNACKTRACK_COLORS[\"secondary\"], linewidth=2)\n",
    "ax.axvline(best_epoch, color=SNACKTRACK_COLORS[\"accent\"], linestyle=\"--\",\n",
    "           alpha=0.7, label=f\"Best epoch ({best_epoch})\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE Loss\")\n",
    "ax.set_title(\"Training vs Validation Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Learning rate schedule ---\n",
    "ax = axes[1]\n",
    "ax.plot(epochs_range, lr_history, color=SNACKTRACK_COLORS[\"accent\"], linewidth=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Learning Rate\")\n",
    "ax.set_title(\"Learning Rate Schedule\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also use the utility function for a standalone loss plot\n",
    "fig2 = plot_loss_curves(train_losses, val_losses,\n",
    "                        title=\"RNN/GRU Training Progress\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Quality\n",
    "\n",
    "We evaluate the trained model on the validation set by:\n",
    "\n",
    "1. **Cosine similarity** between predicted and actual next-meal embeddings ---\n",
    "   this is the metric that directly affects recommendation quality in production,\n",
    "   since the RNN output is matched against recipe embeddings via cosine similarity.\n",
    "2. **Sample predictions** showing side-by-side predicted vs actual vectors.\n",
    "3. **Histogram** of cosine similarities across the full validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Compute predictions on validation set\n",
    "# ---------------------------------------------------------------------------\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        preds = model(X_batch).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_actuals.append(y_batch.numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_actuals = np.concatenate(all_actuals, axis=0)\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_similarity_batch(a, b):\n",
    "    \"\"\"Compute row-wise cosine similarity between two matrices.\"\"\"\n",
    "    dot = np.sum(a * b, axis=1)\n",
    "    norm_a = np.linalg.norm(a, axis=1)\n",
    "    norm_b = np.linalg.norm(b, axis=1)\n",
    "    denom = norm_a * norm_b\n",
    "    denom = np.where(denom > 0, denom, 1.0)\n",
    "    return dot / denom\n",
    "\n",
    "cos_sims = cosine_similarity_batch(all_preds, all_actuals)\n",
    "\n",
    "print(\"Prediction Quality on Validation Set\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"  Mean cosine similarity:   {np.mean(cos_sims):.4f}\")\n",
    "print(f\"  Median cosine similarity: {np.median(cos_sims):.4f}\")\n",
    "print(f\"  Std:                      {np.std(cos_sims):.4f}\")\n",
    "print(f\"  Min / Max:                {np.min(cos_sims):.4f} / {np.max(cos_sims):.4f}\")\n",
    "print(f\"  % > 0.5:                  {100 * np.mean(cos_sims > 0.5):.1f}%\")\n",
    "print(f\"  % > 0.8:                  {100 * np.mean(cos_sims > 0.8):.1f}%\")\n",
    "print(f\"  MSE (val):                {np.mean((all_preds - all_actuals) ** 2):.6f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Sample predictions\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n--- Sample Predictions (first 5) ---\")\n",
    "for i in range(min(5, len(all_preds))):\n",
    "    sim = cos_sims[i]\n",
    "    print(f\"\\nSample {i + 1} | Cosine Similarity: {sim:.4f}\")\n",
    "    print(f\"  Predicted: [{', '.join(f'{v:.3f}' for v in all_preds[i][:8])}  ...]\")\n",
    "    print(f\"  Actual:    [{', '.join(f'{v:.3f}' for v in all_actuals[i][:8])}  ...]\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Histogram of cosine similarities\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "ax.hist(cos_sims, bins=60, color=SNACKTRACK_COLORS[\"primary\"], alpha=0.8,\n",
    "        edgecolor=\"white\", linewidth=0.5)\n",
    "ax.axvline(np.mean(cos_sims), color=SNACKTRACK_COLORS[\"secondary\"], linestyle=\"--\",\n",
    "           linewidth=2, label=f\"Mean = {np.mean(cos_sims):.3f}\")\n",
    "ax.axvline(np.median(cos_sims), color=SNACKTRACK_COLORS[\"accent\"], linestyle=\":\",\n",
    "           linewidth=2, label=f\"Median = {np.median(cos_sims):.3f}\")\n",
    "ax.set_xlabel(\"Cosine Similarity\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Distribution of Cosine Similarity (Predicted vs Actual)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot of predicted vs actual (first 2 dims)\n",
    "ax = axes[1]\n",
    "n_plot = min(500, len(all_preds))\n",
    "ax.scatter(all_actuals[:n_plot, 0], all_preds[:n_plot, 0],\n",
    "           alpha=0.3, s=15, color=SNACKTRACK_COLORS[\"primary\"], label=\"Dim 0\")\n",
    "ax.scatter(all_actuals[:n_plot, 1], all_preds[:n_plot, 1],\n",
    "           alpha=0.3, s=15, color=SNACKTRACK_COLORS[\"secondary\"], label=\"Dim 1\")\n",
    "lims = [min(all_actuals[:n_plot, :2].min(), all_preds[:n_plot, :2].min()) - 0.1,\n",
    "        max(all_actuals[:n_plot, :2].max(), all_preds[:n_plot, :2].max()) + 0.1]\n",
    "ax.plot(lims, lims, \"k--\", alpha=0.5, label=\"Perfect prediction\")\n",
    "ax.set_xlabel(\"Actual\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "ax.set_title(\"Predicted vs Actual (First 2 Embedding Dims)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Weights\n",
    "\n",
    "We export the trained weights to `weights/rnn_weights.npz` using `save_rnn_weights()`.\n",
    "\n",
    "**Shape mapping from PyTorch to NumPy:**\n",
    "\n",
    "| PyTorch Parameter | Export Key | Shape | Notes |\n",
    "|-------------------|-----------|-------|-------|\n",
    "| `gru_cell.Wz` | `Wz` | (39, 64) | Direct copy --- same layout |\n",
    "| `gru_cell.Uz` | `Uz` | (64, 64) | Direct copy |\n",
    "| `gru_cell.bz` | `bz` | (64,) | Direct copy |\n",
    "| ... | ... | ... | Same for Wr, Ur, br, Wh, Uh, bh |\n",
    "| `output_proj.weight` | `Wo` | (64, 32) | **Transposed** --- `nn.Linear` stores `(out, in)` |\n",
    "| `output_proj.bias` | `bo` | (32,) | Direct copy |\n",
    "\n",
    "The critical detail is the transpose on `output_proj.weight`: PyTorch's `nn.Linear`\n",
    "stores the weight matrix as `(output_dim, input_dim)` and computes `x @ W.T + b`,\n",
    "but our production NumPy code computes `h @ Wo + bo` where `Wo` is `(64, 32)`.\n",
    "So we must export `weight.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Extract and export weights\n",
    "# ---------------------------------------------------------------------------\n",
    "model.eval()\n",
    "gru = model.gru_cell\n",
    "\n",
    "weights = {\n",
    "    # GRU gate weights --- direct copy (no transpose needed)\n",
    "    \"Wz\": gru.Wz.detach().cpu().numpy(),   # (39, 64)\n",
    "    \"Uz\": gru.Uz.detach().cpu().numpy(),   # (64, 64)\n",
    "    \"bz\": gru.bz.detach().cpu().numpy(),   # (64,)\n",
    "    \"Wr\": gru.Wr.detach().cpu().numpy(),   # (39, 64)\n",
    "    \"Ur\": gru.Ur.detach().cpu().numpy(),   # (64, 64)\n",
    "    \"br\": gru.br.detach().cpu().numpy(),   # (64,)\n",
    "    \"Wh\": gru.Wh.detach().cpu().numpy(),   # (39, 64)\n",
    "    \"Uh\": gru.Uh.detach().cpu().numpy(),   # (64, 64)\n",
    "    \"bh\": gru.bh.detach().cpu().numpy(),   # (64,)\n",
    "    # Output projection --- TRANSPOSE required\n",
    "    \"Wo\": model.output_proj.weight.detach().cpu().numpy().T,  # (64, 32)\n",
    "    \"bo\": model.output_proj.bias.detach().cpu().numpy(),       # (32,)\n",
    "}\n",
    "\n",
    "# Verify shapes before saving\n",
    "print(\"Exported weight shapes:\")\n",
    "for key, arr in weights.items():\n",
    "    expected = RNN_WEIGHT_SHAPES[key]\n",
    "    status = \"OK\" if arr.shape == expected else \"MISMATCH\"\n",
    "    print(f\"  {key:<5} {str(arr.shape):<12} expected {str(expected):<12} [{status}]\")\n",
    "\n",
    "# Save\n",
    "save_path = save_rnn_weights(weights)\n",
    "print(f\"\\nWeights saved to: {save_path}\")\n",
    "print(f\"File size: {save_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verification\n",
    "\n",
    "This is the most critical step: we load the exported `.npz` weights, replicate the\n",
    "production NumPy GRU forward pass, and verify that the outputs match the PyTorch\n",
    "model's outputs to within floating-point tolerance (`atol=1e-5`).\n",
    "\n",
    "This ensures that when the production `MealSequenceRNN` loads these weights,\n",
    "it will produce **identical** predictions to the trained PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Load exported weights\n",
    "# ---------------------------------------------------------------------------\n",
    "loaded_weights = load_rnn_weights()\n",
    "print(\"Loaded weights:\")\n",
    "for key, arr in loaded_weights.items():\n",
    "    print(f\"  {key:<5} {arr.shape}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# NumPy GRU forward pass (exact copy of production code)\n",
    "# ---------------------------------------------------------------------------\n",
    "def numpy_sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))\n",
    "\n",
    "\n",
    "def numpy_gru_step(x, h_prev, w):\n",
    "    \"\"\"Single GRU step using NumPy --- mirrors MealSequenceRNN.gru_step().\"\"\"\n",
    "    z = numpy_sigmoid(x @ w[\"Wz\"] + h_prev @ w[\"Uz\"] + w[\"bz\"])\n",
    "    r = numpy_sigmoid(x @ w[\"Wr\"] + h_prev @ w[\"Ur\"] + w[\"br\"])\n",
    "    h_candidate = np.tanh(x @ w[\"Wh\"] + (r * h_prev) @ w[\"Uh\"] + w[\"bh\"])\n",
    "    h_new = (1 - z) * h_prev + z * h_candidate\n",
    "    return h_new\n",
    "\n",
    "\n",
    "def numpy_forward(sequence, w):\n",
    "    \"\"\"Full forward pass using NumPy --- mirrors MealSequenceRNN.forward().\"\"\"\n",
    "    h = np.zeros(64, dtype=np.float64)\n",
    "    for x in sequence:\n",
    "        h = numpy_gru_step(x.astype(np.float64), h, w)\n",
    "    output = h @ w[\"Wo\"] + w[\"bo\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Compare PyTorch vs NumPy on validation samples\n",
    "# ---------------------------------------------------------------------------\n",
    "model.eval()\n",
    "n_test = min(20, len(X_val))\n",
    "max_abs_diff = 0.0\n",
    "all_close = True\n",
    "\n",
    "print(f\"\\nVerifying PyTorch vs NumPy on {n_test} validation sequences...\")\n",
    "print(f\"{'Sample':<8} {'Max |diff|':<15} {'Cosine sim':<15} {'Match?'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(n_test):\n",
    "    seq = X_val[i]  # (20, 39)\n",
    "\n",
    "    # PyTorch prediction\n",
    "    with torch.no_grad():\n",
    "        seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        pt_output = model(seq_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # NumPy prediction\n",
    "    np_output = numpy_forward(seq, loaded_weights)\n",
    "\n",
    "    # Compare\n",
    "    abs_diff = np.max(np.abs(pt_output - np_output))\n",
    "    max_abs_diff = max(max_abs_diff, abs_diff)\n",
    "\n",
    "    cos_sim = np.dot(pt_output, np_output) / (\n",
    "        np.linalg.norm(pt_output) * np.linalg.norm(np_output) + 1e-10\n",
    "    )\n",
    "\n",
    "    match = np.allclose(pt_output, np_output, atol=1e-5)\n",
    "    if not match:\n",
    "        all_close = False\n",
    "\n",
    "    print(f\"  {i + 1:<6} {abs_diff:<15.8f} {cos_sim:<15.8f} {'PASS' if match else 'FAIL'}\")\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Maximum absolute difference: {max_abs_diff:.2e}\")\n",
    "print(f\"Overall verification: {'PASSED' if all_close else 'FAILED'}\")\n",
    "\n",
    "if all_close:\n",
    "    print(\"\\nThe exported weights reproduce the PyTorch model's outputs exactly.\")\n",
    "    print(\"The production NumPy RNN will produce identical recommendations.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Numerical differences exceed tolerance!\")\n",
    "    print(\"This is likely due to float32 vs float64 precision.\")\n",
    "    print(f\"Max diff of {max_abs_diff:.2e} may still be acceptable for recommendations.\")\n",
    "\n",
    "# Final assertion\n",
    "for i in range(min(5, len(X_val))):\n",
    "    seq = X_val[i]\n",
    "    with torch.no_grad():\n",
    "        seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        pt_output = model(seq_tensor).cpu().numpy().flatten()\n",
    "    np_output = numpy_forward(seq, loaded_weights)\n",
    "    assert np.allclose(pt_output, np_output, atol=1e-5), (\n",
    "        f\"Verification FAILED on sample {i}: \"\n",
    "        f\"max diff = {np.max(np.abs(pt_output - np_output)):.2e}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nAll assertions passed. Weights are verified and ready for production.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}