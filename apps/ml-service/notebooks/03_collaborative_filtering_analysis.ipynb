{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnackTrack ML — Collaborative Filtering Analysis\n",
    "\n",
    "This notebook explores user-recipe interaction data to understand the collaborative\n",
    "filtering landscape for SnackTrack. We examine the interaction matrix structure,\n",
    "user-user similarities, cold-start characteristics, and generate sample\n",
    "recommendations to validate our approach before integrating into the production\n",
    "hybrid recommender.\n",
    "\n",
    "**Key questions:**\n",
    "- How sparse is our user-recipe interaction matrix?\n",
    "- Can we find meaningful user neighborhoods via cosine similarity?\n",
    "- How many interactions does a user need before collaborative filtering becomes viable?\n",
    "- What do sample recommendations look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "from notebooks.utils.plot_helpers import setup_plot_style\n",
    "\n",
    "setup_plot_style()\n",
    "\n",
    "print(\"Imports ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Interaction Data\n",
    "\n",
    "We attempt to load user-recipe interactions from the SnackTrack database first.\n",
    "If the database is unavailable or empty, we fall back to the Food.com dataset\n",
    "which provides a rich set of user-recipe ratings for offline analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.data_loader import (\n",
    "    load_interactions_from_db,\n",
    "    load_kaggle_dataset,\n",
    ")\n",
    "from notebooks.utils.db_connect import get_connection\n",
    "\n",
    "interactions_df = None\n",
    "data_source = None\n",
    "\n",
    "# ----- Attempt 1: Load from database -----\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    db_interactions = load_interactions_from_db(conn)\n",
    "    conn.close()\n",
    "    if not db_interactions.empty and len(db_interactions) >= 100:\n",
    "        # Map interaction_value to a rating-like score\n",
    "        interactions_df = db_interactions.rename(\n",
    "            columns={\"interaction_value\": \"rating\"}\n",
    "        )[[\"user_id\", \"recipe_id\", \"rating\"]]\n",
    "        data_source = \"SnackTrack DB\"\n",
    "        print(f\"Loaded {len(interactions_df)} interactions from the database.\")\n",
    "except Exception as e:\n",
    "    print(f\"Database unavailable: {e}\")\n",
    "\n",
    "# ----- Attempt 2: Fall back to Food.com dataset -----\n",
    "if interactions_df is None:\n",
    "    try:\n",
    "        raw = load_kaggle_dataset(\"food_com_interactions\")\n",
    "        # Food.com columns: user_id, recipe_id, date, rating, review\n",
    "        interactions_df = raw[[\"user_id\", \"recipe_id\", \"rating\"]].copy()\n",
    "        # Remove zero-ratings (\"I haven't made this\") to focus on actual feedback\n",
    "        interactions_df = interactions_df[interactions_df[\"rating\"] > 0]\n",
    "        data_source = \"Food.com (Kaggle)\"\n",
    "        print(f\"Loaded {len(interactions_df)} interactions from Food.com dataset.\")\n",
    "    except FileNotFoundError:\n",
    "        # ----- Attempt 3: Generate synthetic data for demonstration -----\n",
    "        print(\"No dataset found. Generating synthetic interactions for demonstration.\")\n",
    "        rng = np.random.default_rng(42)\n",
    "        n_users, n_recipes = 500, 2000\n",
    "        n_interactions = 25000\n",
    "        interactions_df = pd.DataFrame({\n",
    "            \"user_id\": rng.integers(0, n_users, size=n_interactions),\n",
    "            \"recipe_id\": rng.integers(0, n_recipes, size=n_interactions),\n",
    "            \"rating\": rng.choice([1, 2, 3, 4, 5], size=n_interactions,\n",
    "                                 p=[0.05, 0.10, 0.20, 0.35, 0.30]),\n",
    "        })\n",
    "        # Deduplicate (keep last rating per user-recipe pair)\n",
    "        interactions_df = interactions_df.drop_duplicates(\n",
    "            subset=[\"user_id\", \"recipe_id\"], keep=\"last\"\n",
    "        ).reset_index(drop=True)\n",
    "        data_source = \"Synthetic\"\n",
    "\n",
    "print(f\"\\nData source: {data_source}\")\n",
    "print(f\"Shape: {interactions_df.shape}\")\n",
    "print(f\"Unique users:   {interactions_df['user_id'].nunique():,}\")\n",
    "print(f\"Unique recipes: {interactions_df['recipe_id'].nunique():,}\")\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(interactions_df[\"rating\"].describe().round(2))\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build User-Recipe Interaction Matrix\n",
    "\n",
    "We pivot the interaction data into a user-by-recipe matrix. For large datasets\n",
    "(Food.com has ~200k users and ~200k recipes), we use a sparse matrix\n",
    "representation to keep memory usage manageable. The sparsity percentage tells us\n",
    "how much of the matrix is unfilled -- typically >99% for recommendation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ID-to-index mappings\n",
    "user_ids = sorted(interactions_df[\"user_id\"].unique())\n",
    "recipe_ids = sorted(interactions_df[\"recipe_id\"].unique())\n",
    "\n",
    "user_to_idx = {uid: i for i, uid in enumerate(user_ids)}\n",
    "recipe_to_idx = {rid: i for i, rid in enumerate(recipe_ids)}\n",
    "idx_to_user = {i: uid for uid, i in user_to_idx.items()}\n",
    "idx_to_recipe = {i: rid for rid, i in recipe_to_idx.items()}\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_recipes = len(recipe_ids)\n",
    "\n",
    "# Build sparse matrix (COO format, then convert to CSR for fast row slicing)\n",
    "row_indices = interactions_df[\"user_id\"].map(user_to_idx).values\n",
    "col_indices = interactions_df[\"recipe_id\"].map(recipe_to_idx).values\n",
    "values = interactions_df[\"rating\"].values.astype(np.float32)\n",
    "\n",
    "interaction_matrix = sparse.csr_matrix(\n",
    "    (values, (row_indices, col_indices)),\n",
    "    shape=(n_users, n_recipes),\n",
    ")\n",
    "\n",
    "total_cells = n_users * n_recipes\n",
    "filled_cells = interaction_matrix.nnz\n",
    "sparsity = 1.0 - (filled_cells / total_cells)\n",
    "\n",
    "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Non-zero entries:         {filled_cells:,}\")\n",
    "print(f\"Total cells:              {total_cells:,}\")\n",
    "print(f\"Sparsity:                 {sparsity:.4%}\")\n",
    "print(f\"Memory (sparse CSR):      {interaction_matrix.data.nbytes / 1024:.1f} KB\")\n",
    "print(f\"Memory (dense equiv):     {total_cells * 4 / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sparsity Visualization\n",
    "\n",
    "Visualizing the matrix structure helps us understand how interactions are\n",
    "distributed. We examine:\n",
    "1. A spy plot showing the non-zero pattern of a subset of the matrix\n",
    "2. Histograms of interaction counts per user and per recipe (on log scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# --- Spy plot of a subset (top 100 users x first 200 recipes) ---\n",
    "subset_rows = min(100, n_users)\n",
    "subset_cols = min(200, n_recipes)\n",
    "subset = interaction_matrix[:subset_rows, :subset_cols].toarray()\n",
    "\n",
    "axes[0].spy(subset, markersize=1, aspect=\"auto\", color=\"#4CAF50\")\n",
    "axes[0].set_title(f\"Interaction Pattern\\n(top {subset_rows} users x {subset_cols} recipes)\")\n",
    "axes[0].set_xlabel(\"Recipe Index\")\n",
    "axes[0].set_ylabel(\"User Index\")\n",
    "\n",
    "# --- Interactions per user (log scale) ---\n",
    "interactions_per_user = np.array(interaction_matrix.getnnz(axis=1)).flatten()\n",
    "axes[1].hist(interactions_per_user, bins=50, color=\"#2196F3\", alpha=0.7, edgecolor=\"white\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].set_title(\"Interactions per User\")\n",
    "axes[1].set_xlabel(\"Number of Interactions\")\n",
    "axes[1].set_ylabel(\"Count (log scale)\")\n",
    "axes[1].axvline(np.median(interactions_per_user), color=\"#F44336\", linestyle=\"--\",\n",
    "                label=f\"Median: {np.median(interactions_per_user):.0f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "# --- Interactions per recipe (log scale) ---\n",
    "interactions_per_recipe = np.array(interaction_matrix.getnnz(axis=0)).flatten()\n",
    "axes[2].hist(interactions_per_recipe, bins=50, color=\"#FF9800\", alpha=0.7, edgecolor=\"white\")\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].set_title(\"Interactions per Recipe\")\n",
    "axes[2].set_xlabel(\"Number of Interactions\")\n",
    "axes[2].set_ylabel(\"Count (log scale)\")\n",
    "axes[2].axvline(np.median(interactions_per_recipe), color=\"#F44336\", linestyle=\"--\",\n",
    "                label=f\"Median: {np.median(interactions_per_recipe):.0f}\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Interactions per user  — mean: {interactions_per_user.mean():.1f}, \"\n",
    "      f\"median: {np.median(interactions_per_user):.0f}, \"\n",
    "      f\"max: {interactions_per_user.max()}\")\n",
    "print(f\"Interactions per recipe — mean: {interactions_per_recipe.mean():.1f}, \"\n",
    "      f\"median: {np.median(interactions_per_recipe):.0f}, \"\n",
    "      f\"max: {interactions_per_recipe.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. User-User Similarity\n",
    "\n",
    "Collaborative filtering relies on finding users with similar taste profiles.\n",
    "We compute cosine similarity between the top-50 most active users to examine\n",
    "whether meaningful clusters emerge. More active users typically produce more\n",
    "reliable similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top-50 most active users\n",
    "top_k = 50\n",
    "most_active_idx = np.argsort(interactions_per_user)[::-1][:top_k]\n",
    "active_matrix = interaction_matrix[most_active_idx].toarray()\n",
    "\n",
    "# Compute cosine similarity\n",
    "user_sim = cosine_similarity(active_matrix)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Heatmap ---\n",
    "sns.heatmap(\n",
    "    user_sim,\n",
    "    ax=axes[0],\n",
    "    cmap=\"YlOrRd\",\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"label\": \"Cosine Similarity\"},\n",
    ")\n",
    "axes[0].set_title(f\"User-User Cosine Similarity\\n(top {top_k} most active users)\")\n",
    "axes[0].set_xlabel(\"User\")\n",
    "axes[0].set_ylabel(\"User\")\n",
    "\n",
    "# --- Distribution of off-diagonal similarity scores ---\n",
    "mask = np.triu(np.ones_like(user_sim, dtype=bool), k=1)\n",
    "off_diag = user_sim[mask]\n",
    "\n",
    "axes[1].hist(off_diag, bins=50, color=\"#9C27B0\", alpha=0.7, edgecolor=\"white\")\n",
    "axes[1].set_title(\"Distribution of Pairwise Similarities\")\n",
    "axes[1].set_xlabel(\"Cosine Similarity\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].axvline(off_diag.mean(), color=\"#F44336\", linestyle=\"--\",\n",
    "                label=f\"Mean: {off_diag.mean():.3f}\")\n",
    "axes[1].axvline(np.median(off_diag), color=\"#2196F3\", linestyle=\"--\",\n",
    "                label=f\"Median: {np.median(off_diag):.3f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Similarity stats (off-diagonal, top-{top_k} users):\")\n",
    "print(f\"  Mean:   {off_diag.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(off_diag):.4f}\")\n",
    "print(f\"  Std:    {off_diag.std():.4f}\")\n",
    "print(f\"  Min:    {off_diag.min():.4f}\")\n",
    "print(f\"  Max:    {off_diag.max():.4f}\")\n",
    "print(f\"  Pairs with sim > 0.1: {(off_diag > 0.1).sum()} / {len(off_diag)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cold-Start Analysis\n",
    "\n",
    "A critical challenge in collaborative filtering is the **cold-start problem**:\n",
    "new users with few interactions cannot be reliably matched to similar users.\n",
    "\n",
    "Here we simulate cold-start conditions by selecting users with varying numbers\n",
    "of interactions and measuring how many \"effective neighbors\" they have\n",
    "(i.e., users with cosine similarity > 0.1). This informs our production\n",
    "`COLD_START_THRESHOLD` setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a reference set from the top-200 most active users\n",
    "ref_k = min(200, n_users)\n",
    "ref_idx = np.argsort(interactions_per_user)[::-1][:ref_k]\n",
    "ref_matrix = interaction_matrix[ref_idx].toarray()\n",
    "\n",
    "# Interaction count thresholds to test\n",
    "thresholds = [1, 3, 5, 10, 20, 50]\n",
    "similarity_cutoff = 0.1\n",
    "\n",
    "results = []\n",
    "n_trials = 20  # number of users to sample per threshold\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Find users with approximately this many interactions\n",
    "    eligible = np.where(\n",
    "        (interactions_per_user >= threshold) &\n",
    "        (interactions_per_user < threshold * 3)\n",
    "    )[0]\n",
    "\n",
    "    if len(eligible) == 0:\n",
    "        # If no users match the range, take users closest to the threshold\n",
    "        eligible = np.argsort(np.abs(interactions_per_user - threshold))[:n_trials]\n",
    "\n",
    "    sample_idx = rng.choice(eligible, size=min(n_trials, len(eligible)), replace=False)\n",
    "    \n",
    "    neighbor_counts = []\n",
    "    for idx in sample_idx:\n",
    "        user_vec = interaction_matrix[idx].toarray()\n",
    "        sims = cosine_similarity(user_vec, ref_matrix)[0]\n",
    "        # Exclude self if in reference set\n",
    "        effective_neighbors = (sims > similarity_cutoff).sum()\n",
    "        neighbor_counts.append(effective_neighbors)\n",
    "\n",
    "    results.append({\n",
    "        \"interaction_count\": threshold,\n",
    "        \"mean_neighbors\": np.mean(neighbor_counts),\n",
    "        \"std_neighbors\": np.std(neighbor_counts),\n",
    "        \"median_neighbors\": np.median(neighbor_counts),\n",
    "        \"min_neighbors\": np.min(neighbor_counts),\n",
    "        \"max_neighbors\": np.max(neighbor_counts),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.errorbar(\n",
    "    results_df[\"interaction_count\"],\n",
    "    results_df[\"mean_neighbors\"],\n",
    "    yerr=results_df[\"std_neighbors\"],\n",
    "    marker=\"o\",\n",
    "    color=\"#4CAF50\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    capsize=5,\n",
    "    label=\"Mean +/- Std\",\n",
    ")\n",
    "ax.plot(\n",
    "    results_df[\"interaction_count\"],\n",
    "    results_df[\"median_neighbors\"],\n",
    "    marker=\"s\",\n",
    "    color=\"#FF9800\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    linestyle=\"--\",\n",
    "    label=\"Median\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"User Interaction Count\")\n",
    "ax.set_ylabel(f\"Effective Neighbors (sim > {similarity_cutoff})\")\n",
    "ax.set_title(\"Cold-Start Analysis: Effective Neighbors vs. Interaction Count\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark the production cold-start threshold (from SnackTrack config)\n",
    "ax.axvline(5, color=\"#F44336\", linestyle=\":\", alpha=0.7,\n",
    "           label=\"Production threshold (5)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendation Generation\n",
    "\n",
    "We demonstrate the full collaborative filtering pipeline for a sample user:\n",
    "1. Find the K most similar users (by cosine similarity)\n",
    "2. Aggregate their ratings weighted by similarity\n",
    "3. Exclude recipes the target user has already rated\n",
    "4. Return the top-10 recommended recipes\n",
    "\n",
    "This mirrors the logic in `app/recommender/collaborative.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_recommend(user_idx, interaction_mat, k_neighbors=20, top_n=10):\n",
    "    \"\"\"Generate collaborative filtering recommendations for a single user.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user_idx : int\n",
    "        Index of the target user in the interaction matrix.\n",
    "    interaction_mat : sparse matrix\n",
    "        User-recipe interaction matrix (CSR format).\n",
    "    k_neighbors : int\n",
    "        Number of similar users to consider.\n",
    "    top_n : int\n",
    "        Number of recommendations to return.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    recommendations : list of (recipe_idx, predicted_score)\n",
    "    neighbor_sims : array of similarity scores for the chosen neighbors\n",
    "    \"\"\"\n",
    "    user_vec = interaction_mat[user_idx].toarray().flatten()\n",
    "    \n",
    "    # Compute similarity to all other users\n",
    "    all_sims = cosine_similarity(\n",
    "        user_vec.reshape(1, -1),\n",
    "        interaction_mat.toarray()\n",
    "    )[0]\n",
    "    \n",
    "    # Zero out self-similarity\n",
    "    all_sims[user_idx] = 0\n",
    "    \n",
    "    # Select top-K neighbors with positive similarity\n",
    "    neighbor_idx = np.argsort(all_sims)[::-1][:k_neighbors]\n",
    "    neighbor_sims = all_sims[neighbor_idx]\n",
    "    \n",
    "    # Keep only neighbors with positive similarity\n",
    "    positive_mask = neighbor_sims > 0\n",
    "    neighbor_idx = neighbor_idx[positive_mask]\n",
    "    neighbor_sims = neighbor_sims[positive_mask]\n",
    "    \n",
    "    if len(neighbor_idx) == 0:\n",
    "        return [], np.array([])\n",
    "    \n",
    "    # Weighted aggregation of neighbor ratings\n",
    "    neighbor_matrix = interaction_mat[neighbor_idx].toarray()\n",
    "    weighted_scores = neighbor_sims @ neighbor_matrix  # (k,) @ (k, n_recipes) -> (n_recipes,)\n",
    "    sim_sum = neighbor_sims.sum()\n",
    "    if sim_sum > 0:\n",
    "        weighted_scores /= sim_sum  # normalize by total similarity\n",
    "    \n",
    "    # Exclude recipes the user has already rated\n",
    "    already_rated = np.where(user_vec > 0)[0]\n",
    "    weighted_scores[already_rated] = 0\n",
    "    \n",
    "    # Get top-N\n",
    "    top_recipe_idx = np.argsort(weighted_scores)[::-1][:top_n]\n",
    "    recommendations = [\n",
    "        (int(ridx), float(weighted_scores[ridx]))\n",
    "        for ridx in top_recipe_idx\n",
    "        if weighted_scores[ridx] > 0\n",
    "    ]\n",
    "    \n",
    "    return recommendations, neighbor_sims\n",
    "\n",
    "\n",
    "# Pick a sample user who has a reasonable number of interactions\n",
    "target_interaction_count = 15\n",
    "candidate_users = np.where(\n",
    "    (interactions_per_user >= target_interaction_count) &\n",
    "    (interactions_per_user <= target_interaction_count * 5)\n",
    ")[0]\n",
    "\n",
    "if len(candidate_users) == 0:\n",
    "    # Fallback: pick the user closest to the target count\n",
    "    sample_user_idx = np.argmin(np.abs(interactions_per_user - target_interaction_count))\n",
    "else:\n",
    "    sample_user_idx = candidate_users[0]\n",
    "\n",
    "sample_user_id = idx_to_user[sample_user_idx]\n",
    "sample_user_ratings = interactions_df[interactions_df[\"user_id\"] == sample_user_id]\n",
    "\n",
    "print(f\"Sample user: {sample_user_id}\")\n",
    "print(f\"Number of interactions: {interactions_per_user[sample_user_idx]}\")\n",
    "print(f\"\\nUser's existing ratings:\")\n",
    "print(sample_user_ratings.head(10).to_string(index=False))\n",
    "\n",
    "# Generate recommendations\n",
    "recs, neighbor_sims = collaborative_recommend(\n",
    "    sample_user_idx, interaction_matrix, k_neighbors=20, top_n=10\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Top-10 Collaborative Filtering Recommendations\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Rank':<6} {'Recipe ID':<15} {'Predicted Score':<15}\")\n",
    "print(f\"{'-'*6} {'-'*15} {'-'*15}\")\n",
    "\n",
    "for rank, (ridx, score) in enumerate(recs, 1):\n",
    "    recipe_id = idx_to_recipe[ridx]\n",
    "    print(f\"{rank:<6} {str(recipe_id):<15} {score:<15.4f}\")\n",
    "\n",
    "print(f\"\\nNeighbors used: {len(neighbor_sims)}\")\n",
    "if len(neighbor_sims) > 0:\n",
    "    print(f\"Neighbor similarity range: [{neighbor_sims.min():.4f}, {neighbor_sims.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings\n",
    "\n",
    "Summary statistics and observations from the collaborative filtering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"  COLLABORATIVE FILTERING ANALYSIS — KEY FINDINGS\")\n",
    "print(\"=\" * 65)\n",
    "print()\n",
    "print(f\"Data source:              {data_source}\")\n",
    "print(f\"Total interactions:       {len(interactions_df):,}\")\n",
    "print(f\"Unique users:             {n_users:,}\")\n",
    "print(f\"Unique recipes:           {n_recipes:,}\")\n",
    "print(f\"Matrix sparsity:          {sparsity:.4%}\")\n",
    "print(f\"Mean interactions/user:   {interactions_per_user.mean():.1f}\")\n",
    "print(f\"Median interactions/user: {np.median(interactions_per_user):.0f}\")\n",
    "print(f\"Mean interactions/recipe: {interactions_per_recipe.mean():.1f}\")\n",
    "print()\n",
    "print(\"--- User Similarity (top-50 active users) ---\")\n",
    "print(f\"Mean cosine similarity:   {off_diag.mean():.4f}\")\n",
    "print(f\"Pairs with sim > 0.1:     {(off_diag > 0.1).sum()} / {len(off_diag)}\")\n",
    "print()\n",
    "print(\"--- Cold-Start Insights ---\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"  {int(row['interaction_count']):3d} interactions -> \"\n",
    "          f\"{row['mean_neighbors']:.1f} effective neighbors (avg)\")\n",
    "print()\n",
    "print(\"--- Observations ---\")\n",
    "print(\"1. The interaction matrix is extremely sparse, which is typical\")\n",
    "print(\"   for recommendation systems. Sparse matrices are essential.\")\n",
    "print(\"2. User-user similarity increases with interaction count,\")\n",
    "print(\"   confirming collaborative filtering improves with more data.\")\n",
    "print(\"3. Cold-start users (<5 interactions) have very few effective\")\n",
    "print(\"   neighbors, justifying the production COLD_START_THRESHOLD.\")\n",
    "print(\"4. The hybrid recommender should weight content-based and\")\n",
    "print(\"   knowledge-based signals more heavily for cold-start users,\")\n",
    "print(\"   transitioning to collaborative filtering as data accumulates.\")\n",
    "print()\n",
    "print(\"These findings directly inform the weight blending in\")\n",
    "print(\"app/recommender/hybrid.py.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}