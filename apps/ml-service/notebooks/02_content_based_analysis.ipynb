{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnackTrack ML --- Content-Based Filtering Analysis\n",
    "\n",
    "This notebook builds and evaluates the **content-based filtering** pipeline that\n",
    "powers SnackTrack's recipe recommendations. We walk through every stage:\n",
    "\n",
    "1. **Ingredient vectorization** -- TF-IDF + TruncatedSVD to produce 128-D ingredient vectors\n",
    "   (matching the `ingredient_vector` column in the `recipes` table)\n",
    "2. **Latent space visualization** -- t-SNE projections colored by cuisine\n",
    "3. **User preference vectors** -- Weighted aggregation of recipe vectors from interactions\n",
    "   (replicating `retrain_user_model()` from `app/recommender/hybrid.py`)\n",
    "4. **Weight sensitivity** -- How changes in `INTERACTION_WEIGHTS` affect preference vectors\n",
    "5. **Similarity sanity check** -- Top-K retrieval using cosine similarity\n",
    "6. **Nutrition vector analysis** -- 12-D nutrition features, PCA/t-SNE, KMeans clustering\n",
    "\n",
    "> **Prerequisites**: Run notebooks `00` and `01` first so that Parquet datasets are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Allow imports from the parent directory\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from notebooks.utils.plot_helpers import (\n",
    "    setup_plot_style,\n",
    "    plot_latent_space_2d,\n",
    "    plot_feature_distributions,\n",
    "    SNACKTRACK_COLORS,\n",
    "    PALETTE,\n",
    ")\n",
    "from notebooks.utils.data_loader import load_kaggle_dataset, extract_vae_features\n",
    "\n",
    "setup_plot_style()\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Ingredient Vectors\n",
    "\n",
    "The `recipe_ingredients` Kaggle dataset contains ~40K recipes with:\n",
    "- A list of **ingredients** (free-text)\n",
    "- A **cuisine** label\n",
    "\n",
    "We transform the ingredient lists into dense 128-dimensional vectors using:\n",
    "1. **TF-IDF** on the ingredient text (captures ingredient frequency/importance)\n",
    "2. **TruncatedSVD** to reduce to 128 dimensions (matching the DB `ingredient_vector` schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recipe ingredients dataset\n",
    "try:\n",
    "    recipe_ing = load_kaggle_dataset(\"recipe_ingredients\")\n",
    "    print(f\"Loaded recipe_ingredients: {recipe_ing.shape[0]:,} rows, {recipe_ing.shape[1]} columns\")\n",
    "    print(f\"Columns: {list(recipe_ing.columns)}\")\n",
    "    display(recipe_ing.head(3))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset not found: {e}\")\n",
    "    print(\"Please run notebook 00 first.\")\n",
    "    recipe_ing = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128  # Matches DB ingredient_vector dimension\n",
    "\n",
    "if not recipe_ing.empty:\n",
    "    # Identify the ingredients column (may be named 'ingredients' or 'ingredient_list')\n",
    "    ing_col = None\n",
    "    for c in [\"ingredients\", \"ingredient_list\", \"ingredient\"]:\n",
    "        if c in recipe_ing.columns:\n",
    "            ing_col = c\n",
    "            break\n",
    "\n",
    "    # Identify cuisine column\n",
    "    cuisine_col = None\n",
    "    for c in [\"cuisine\", \"cuisine_type\", \"cuisine_types\"]:\n",
    "        if c in recipe_ing.columns:\n",
    "            cuisine_col = c\n",
    "            break\n",
    "\n",
    "    if ing_col is None:\n",
    "        raise ValueError(f\"Could not find ingredients column. Available: {list(recipe_ing.columns)}\")\n",
    "\n",
    "    # Convert ingredient lists to strings if they are stored as lists\n",
    "    def ingredients_to_text(val):\n",
    "        \"\"\"Convert ingredient values to a single text string.\"\"\"\n",
    "        if isinstance(val, list):\n",
    "            return \" \".join(str(v) for v in val)\n",
    "        if isinstance(val, str):\n",
    "            # Handle JSON-encoded lists like '[\"salt\", \"pepper\"]'\n",
    "            if val.startswith(\"[\"):\n",
    "                import json\n",
    "                try:\n",
    "                    return \" \".join(json.loads(val))\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    pass\n",
    "            return val\n",
    "        return str(val)\n",
    "\n",
    "    recipe_ing[\"ingredients_text\"] = recipe_ing[ing_col].apply(ingredients_to_text)\n",
    "\n",
    "    # Drop rows with empty/missing ingredients\n",
    "    recipe_ing = recipe_ing[recipe_ing[\"ingredients_text\"].str.strip().str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"Recipes with valid ingredients: {len(recipe_ing):,}\")\n",
    "\n",
    "    # Step 1: TF-IDF vectorization\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "        max_df=0.95,\n",
    "    )\n",
    "    tfidf_matrix = tfidf.fit_transform(recipe_ing[\"ingredients_text\"])\n",
    "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "    # Step 2: TruncatedSVD to reduce to 128 dimensions\n",
    "    svd = TruncatedSVD(n_components=EMBEDDING_DIM, random_state=42)\n",
    "    ingredient_vectors = svd.fit_transform(tfidf_matrix)\n",
    "    explained_var = svd.explained_variance_ratio_.sum()\n",
    "\n",
    "    print(f\"Ingredient vectors shape: {ingredient_vectors.shape}\")\n",
    "    print(f\"Explained variance: {explained_var:.1%}\")\n",
    "    print(f\"\\nVector statistics:\")\n",
    "    print(f\"  Mean: {ingredient_vectors.mean():.4f}\")\n",
    "    print(f\"  Std:  {ingredient_vectors.std():.4f}\")\n",
    "    print(f\"  Min:  {ingredient_vectors.min():.4f}\")\n",
    "    print(f\"  Max:  {ingredient_vectors.max():.4f}\")\n",
    "    print(f\"  L2 norm (mean): {np.linalg.norm(ingredient_vectors, axis=1).mean():.4f}\")\n",
    "else:\n",
    "    ingredient_vectors = None\n",
    "    print(\"No recipe ingredients data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingredient Space Visualization\n",
    "\n",
    "We use **t-SNE** to project the 128-D ingredient vectors down to 2 dimensions\n",
    "and color each point by its **cuisine type**. This reveals whether ingredient\n",
    "compositions naturally cluster by cuisine -- a prerequisite for effective\n",
    "content-based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ingredient_vectors is not None and cuisine_col is not None:\n",
    "    # Sample for faster t-SNE (full dataset may be too large)\n",
    "    MAX_TSNE_SAMPLES = 5000\n",
    "    n_samples = min(len(ingredient_vectors), MAX_TSNE_SAMPLES)\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    sample_idx = rng.choice(len(ingredient_vectors), size=n_samples, replace=False)\n",
    "\n",
    "    sample_vectors = ingredient_vectors[sample_idx]\n",
    "    sample_cuisines = recipe_ing[cuisine_col].iloc[sample_idx].values\n",
    "\n",
    "    # Keep only top-N cuisines for readable legend\n",
    "    top_cuisines = recipe_ing[cuisine_col].value_counts().head(10).index.tolist()\n",
    "    cuisine_labels = [\n",
    "        c if c in top_cuisines else \"other\"\n",
    "        for c in sample_cuisines\n",
    "    ]\n",
    "\n",
    "    print(f\"Running t-SNE on {n_samples:,} samples...\")\n",
    "    fig = plot_latent_space_2d(\n",
    "        sample_vectors,\n",
    "        labels=cuisine_labels,\n",
    "        title=f\"Ingredient Space (t-SNE, {n_samples:,} recipes, top-10 cuisines)\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # Print cuisine counts in sample\n",
    "    cuisine_series = pd.Series(cuisine_labels)\n",
    "    print(f\"\\nCuisine distribution in sample:\")\n",
    "    print(cuisine_series.value_counts().to_string())\n",
    "elif ingredient_vectors is not None:\n",
    "    print(\"No cuisine column found -- plotting without color labels.\")\n",
    "    MAX_TSNE_SAMPLES = 5000\n",
    "    n_samples = min(len(ingredient_vectors), MAX_TSNE_SAMPLES)\n",
    "    rng = np.random.RandomState(42)\n",
    "    sample_idx = rng.choice(len(ingredient_vectors), size=n_samples, replace=False)\n",
    "    fig = plot_latent_space_2d(\n",
    "        ingredient_vectors[sample_idx],\n",
    "        title=f\"Ingredient Space (t-SNE, {n_samples:,} recipes)\",\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping visualization -- no ingredient vectors available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preference Vector Construction\n",
    "\n",
    "This section replicates the core logic from `retrain_user_model()` in\n",
    "`app/recommender/hybrid.py`. Given a set of user interactions, we compute a\n",
    "**preference vector** as a weighted average of the recipe ingredient vectors.\n",
    "\n",
    "The interaction weights used in production are:\n",
    "\n",
    "| Interaction Type | Weight |\n",
    "|-----------------|--------|\n",
    "| `cook` | 5.0 |\n",
    "| `log` | 4.0 |\n",
    "| `swap_accept` | 3.0 |\n",
    "| `rate` | 1.0 x rating value |\n",
    "| `view` | 1.0 |\n",
    "| `swap_reject` | -2.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production interaction weights (from app/recommender/hybrid.py)\n",
    "INTERACTION_WEIGHTS = {\n",
    "    \"cook\": 5.0,\n",
    "    \"log\": 4.0,\n",
    "    \"swap_accept\": 3.0,\n",
    "    \"rate\": 1.0,   # multiplied by actual rating value\n",
    "    \"view\": 1.0,\n",
    "    \"swap_reject\": -2.0,\n",
    "}\n",
    "\n",
    "\n",
    "def compute_preference_vector(\n",
    "    interactions: list[dict],\n",
    "    recipe_vectors: np.ndarray,\n",
    "    recipe_ids: list,\n",
    "    weights: dict | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a user preference vector from weighted recipe interactions.\n",
    "\n",
    "    Matches the logic in retrain_user_model():\n",
    "    1. Look up the recipe's ingredient vector for each interaction\n",
    "    2. Multiply by the interaction weight (and rating value for 'rate' type)\n",
    "    3. Compute weighted average\n",
    "    4. Normalize to unit vector\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = INTERACTION_WEIGHTS\n",
    "\n",
    "    # Build recipe_id -> vector lookup\n",
    "    id_to_idx = {rid: i for i, rid in enumerate(recipe_ids)}\n",
    "\n",
    "    vectors = []\n",
    "    w_list = []\n",
    "\n",
    "    for interaction in interactions:\n",
    "        rid = interaction[\"recipe_id\"]\n",
    "        if rid not in id_to_idx:\n",
    "            continue\n",
    "\n",
    "        vec = recipe_vectors[id_to_idx[rid]]\n",
    "        itype = interaction[\"interaction_type\"]\n",
    "        ivalue = float(interaction.get(\"interaction_value\", 0) or 0)\n",
    "\n",
    "        w = weights.get(itype, 1.0)\n",
    "        if itype == \"rate\":\n",
    "            w *= ivalue\n",
    "\n",
    "        vectors.append(vec)\n",
    "        w_list.append(w)\n",
    "\n",
    "    if not vectors:\n",
    "        return np.zeros(recipe_vectors.shape[1])\n",
    "\n",
    "    vectors_arr = np.array(vectors)\n",
    "    weights_arr = np.array(w_list).reshape(-1, 1)\n",
    "\n",
    "    # Clip extreme weights\n",
    "    weights_arr = np.clip(weights_arr, -10, 10)\n",
    "    total_weight = np.abs(weights_arr).sum()\n",
    "    if total_weight == 0:\n",
    "        total_weight = 1.0\n",
    "\n",
    "    pref_vec = (vectors_arr * weights_arr).sum(axis=0) / total_weight\n",
    "\n",
    "    # Normalize to unit vector\n",
    "    norm = np.linalg.norm(pref_vec)\n",
    "    if norm > 0:\n",
    "        pref_vec = pref_vec / norm\n",
    "\n",
    "    return pref_vec\n",
    "\n",
    "\n",
    "if ingredient_vectors is not None:\n",
    "    # Create sample recipe IDs\n",
    "    if \"id\" in recipe_ing.columns:\n",
    "        recipe_ids = recipe_ing[\"id\"].tolist()\n",
    "    else:\n",
    "        recipe_ids = list(range(len(recipe_ing)))\n",
    "\n",
    "    # Simulate a user who has interacted with 15 random recipes\n",
    "    rng = np.random.RandomState(123)\n",
    "    sample_recipe_indices = rng.choice(len(recipe_ids), size=15, replace=False)\n",
    "\n",
    "    interaction_types = [\"cook\", \"log\", \"view\", \"rate\", \"swap_accept\", \"view\",\n",
    "                         \"cook\", \"log\", \"view\", \"rate\", \"swap_reject\",\n",
    "                         \"view\", \"cook\", \"log\", \"rate\"]\n",
    "\n",
    "    sample_interactions = []\n",
    "    for i, idx in enumerate(sample_recipe_indices):\n",
    "        interaction = {\n",
    "            \"recipe_id\": recipe_ids[idx],\n",
    "            \"interaction_type\": interaction_types[i],\n",
    "            \"interaction_value\": rng.choice([3, 4, 5]) if interaction_types[i] == \"rate\" else None,\n",
    "        }\n",
    "        sample_interactions.append(interaction)\n",
    "\n",
    "    # Compute preference vector\n",
    "    pref_vec = compute_preference_vector(\n",
    "        sample_interactions, ingredient_vectors, recipe_ids\n",
    "    )\n",
    "\n",
    "    print(f\"Sample user has {len(sample_interactions)} interactions\")\n",
    "    print(f\"Preference vector shape: {pref_vec.shape}\")\n",
    "    print(f\"Preference vector L2 norm: {np.linalg.norm(pref_vec):.4f}\")\n",
    "    print(f\"Non-zero dimensions: {(pref_vec != 0).sum()}/{len(pref_vec)}\")\n",
    "\n",
    "    # Show interaction breakdown\n",
    "    interaction_summary = pd.DataFrame(sample_interactions)\n",
    "    print(f\"\\nInteraction breakdown:\")\n",
    "    print(interaction_summary[\"interaction_type\"].value_counts().to_string())\n",
    "else:\n",
    "    pref_vec = None\n",
    "    print(\"Skipping -- no ingredient vectors available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interaction Weight Sensitivity\n",
    "\n",
    "How sensitive is the resulting preference vector to changes in the interaction weights?\n",
    "We vary each weight individually and measure the **cosine distance** between the\n",
    "resulting preference vector and the baseline. A **heatmap** shows which weights\n",
    "have the most influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ingredient_vectors is not None and pref_vec is not None:\n",
    "    # Weight multipliers to test\n",
    "    multipliers = [0.0, 0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "    interaction_types_to_test = [\"cook\", \"log\", \"swap_accept\", \"rate\", \"view\", \"swap_reject\"]\n",
    "\n",
    "    # Compute baseline\n",
    "    baseline_vec = compute_preference_vector(\n",
    "        sample_interactions, ingredient_vectors, recipe_ids, INTERACTION_WEIGHTS\n",
    "    )\n",
    "\n",
    "    # Build cosine distance matrix\n",
    "    distance_matrix = np.zeros((len(interaction_types_to_test), len(multipliers)))\n",
    "\n",
    "    for i, itype in enumerate(interaction_types_to_test):\n",
    "        for j, mult in enumerate(multipliers):\n",
    "            modified_weights = INTERACTION_WEIGHTS.copy()\n",
    "            modified_weights[itype] = INTERACTION_WEIGHTS[itype] * mult\n",
    "\n",
    "            modified_vec = compute_preference_vector(\n",
    "                sample_interactions, ingredient_vectors, recipe_ids, modified_weights\n",
    "            )\n",
    "\n",
    "            # Cosine distance = 1 - cosine_similarity\n",
    "            if np.linalg.norm(modified_vec) > 0 and np.linalg.norm(baseline_vec) > 0:\n",
    "                cos_sim = cosine_similarity(\n",
    "                    baseline_vec.reshape(1, -1), modified_vec.reshape(1, -1)\n",
    "                )[0, 0]\n",
    "                distance_matrix[i, j] = 1.0 - cos_sim\n",
    "            else:\n",
    "                distance_matrix[i, j] = 1.0\n",
    "\n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(\n",
    "        distance_matrix,\n",
    "        xticklabels=[f\"{m:.1f}x\" for m in multipliers],\n",
    "        yticklabels=interaction_types_to_test,\n",
    "        annot=True,\n",
    "        fmt=\".4f\",\n",
    "        cmap=\"YlOrRd\",\n",
    "        ax=ax,\n",
    "        cbar_kws={\"label\": \"Cosine Distance from Baseline\"},\n",
    "    )\n",
    "    ax.set_xlabel(\"Weight Multiplier\", fontsize=12)\n",
    "    ax.set_ylabel(\"Interaction Type\", fontsize=12)\n",
    "    ax.set_title(\"Preference Vector Sensitivity to Interaction Weights\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify most sensitive weight\n",
    "    max_sensitivity = distance_matrix.max(axis=1)\n",
    "    most_sensitive = interaction_types_to_test[np.argmax(max_sensitivity)]\n",
    "    print(f\"\\nMost sensitive interaction type: '{most_sensitive}' \"\n",
    "          f\"(max cosine distance = {max_sensitivity.max():.4f})\")\n",
    "    print(f\"Least sensitive: '{interaction_types_to_test[np.argmin(max_sensitivity)]}' \"\n",
    "          f\"(max cosine distance = {max_sensitivity.min():.4f})\")\n",
    "else:\n",
    "    print(\"Skipping weight sensitivity analysis -- no vectors available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Content Similarity Sanity Check\n",
    "\n",
    "For a sample preference vector, we retrieve the **top-10 most similar recipes**\n",
    "by cosine similarity and display their names, cuisines, and similarity scores.\n",
    "This replicates the core retrieval step of `get_content_recommendations()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ingredient_vectors is not None and pref_vec is not None:\n",
    "    # Compute cosine similarity between preference vector and all recipes\n",
    "    similarities = cosine_similarity(pref_vec.reshape(1, -1), ingredient_vectors)[0]\n",
    "\n",
    "    # Get top-10 most similar\n",
    "    top_k = 10\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "    # Build results table\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        row = {\"Rank\": rank, \"Similarity\": f\"{similarities[idx]:.4f}\"}\n",
    "\n",
    "        # Try to get recipe name/ID\n",
    "        for name_col in [\"title\", \"name\", \"recipe_name\", \"dish\"]:\n",
    "            if name_col in recipe_ing.columns:\n",
    "                row[\"Recipe\"] = recipe_ing.iloc[idx][name_col]\n",
    "                break\n",
    "        else:\n",
    "            row[\"Recipe\"] = f\"Recipe #{idx}\"\n",
    "\n",
    "        if cuisine_col and cuisine_col in recipe_ing.columns:\n",
    "            row[\"Cuisine\"] = recipe_ing.iloc[idx][cuisine_col]\n",
    "\n",
    "        # Show a snippet of ingredients\n",
    "        ing_text = recipe_ing.iloc[idx].get(\"ingredients_text\", \"\")\n",
    "        row[\"Ingredients (preview)\"] = (ing_text[:80] + \"...\") if len(str(ing_text)) > 80 else ing_text\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Top-{top_k} recipes most similar to sample user's preference vector:\\n\")\n",
    "    display(results_df)\n",
    "\n",
    "    # Distribution of similarity scores\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.hist(similarities, bins=80, color=SNACKTRACK_COLORS[\"primary\"], edgecolor=\"white\", alpha=0.8)\n",
    "    ax.axvline(similarities[top_indices[-1]], color=\"red\", linestyle=\"--\",\n",
    "               linewidth=1.5, label=f\"Top-{top_k} threshold = {similarities[top_indices[-1]]:.3f}\")\n",
    "    ax.set_xlabel(\"Cosine Similarity to Preference Vector\")\n",
    "    ax.set_ylabel(\"Number of Recipes\")\n",
    "    ax.set_title(\"Similarity Score Distribution\", fontsize=13)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping similarity check -- no vectors or preference vector available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Nutrition Vector Analysis\n",
    "\n",
    "Beyond ingredient vectors, SnackTrack uses a **12-dimensional nutrition vector**\n",
    "for each recipe (see `extract_vae_features()` in `utils/data_loader.py`). These\n",
    "features include:\n",
    "\n",
    "| Dim | Feature |\n",
    "|-----|----------|\n",
    "| 0 | calories |\n",
    "| 1 | protein_g |\n",
    "| 2 | carbs_g |\n",
    "| 3 | fat_g |\n",
    "| 4 | sodium_mg |\n",
    "| 5 | fiber_g |\n",
    "| 6 | sugar_g |\n",
    "| 7 | ready_in_minutes |\n",
    "| 8 | servings |\n",
    "| 9 | vegetarian flag |\n",
    "| 10 | vegan flag |\n",
    "| 11 | gluten_free flag |\n",
    "\n",
    "We visualize these features with PCA/t-SNE and apply KMeans clustering to see\n",
    "if natural recipe groupings emerge from nutrition data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset with nutrition columns\n",
    "nutrition_df = None\n",
    "\n",
    "# Try multiple Kaggle sources for nutrition data\n",
    "for ds_name in [\"global_food_nutrition\", \"epicurious\", \"daily_food_nutrition\", \"diet_recommendations\"]:\n",
    "    try:\n",
    "        candidate = load_kaggle_dataset(ds_name)\n",
    "        # Check if it has enough nutrition columns\n",
    "        nutrition_cols_present = sum(\n",
    "            1 for c in [\"calories\", \"protein_g\", \"protein\", \"carbs_g\", \"carbohydrates\",\n",
    "                        \"fat_g\", \"fat\", \"fiber_g\", \"fiber\", \"sugar_g\", \"sugar\"]\n",
    "            if c in candidate.columns\n",
    "        )\n",
    "        if nutrition_cols_present >= 3:\n",
    "            nutrition_df = candidate\n",
    "            print(f\"Using '{ds_name}' for nutrition analysis ({len(nutrition_df):,} rows)\")\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "if nutrition_df is not None:\n",
    "    # Standardize column names for extract_vae_features()\n",
    "    col_renames = {\n",
    "        \"protein\": \"protein_g\",\n",
    "        \"carbohydrates\": \"carbs_g\",\n",
    "        \"carbs\": \"carbs_g\",\n",
    "        \"total_carbohydrate_g\": \"carbs_g\",\n",
    "        \"fat\": \"fat_g\",\n",
    "        \"total_fat_g\": \"fat_g\",\n",
    "        \"fiber\": \"fiber_g\",\n",
    "        \"dietary_fiber_g\": \"fiber_g\",\n",
    "        \"sugar\": \"sugar_g\",\n",
    "        \"sugars_g\": \"sugar_g\",\n",
    "        \"sodium\": \"sodium_mg\",\n",
    "        \"energy_kcal\": \"calories\",\n",
    "    }\n",
    "    nutrition_df = nutrition_df.rename(columns={\n",
    "        c: col_renames[c] for c in nutrition_df.columns if c in col_renames\n",
    "    })\n",
    "\n",
    "    # Extract 12D feature vectors (same logic as production)\n",
    "    features_12d = extract_vae_features(nutrition_df)\n",
    "    print(f\"Extracted 12D nutrition vectors: {features_12d.shape}\")\n",
    "\n",
    "    # Remove rows with all zeros\n",
    "    nonzero_mask = features_12d.sum(axis=1) > 0\n",
    "    features_12d = features_12d[nonzero_mask]\n",
    "    print(f\"After removing zero rows: {features_12d.shape}\")\n",
    "\n",
    "    # Plot feature distributions\n",
    "    feature_names = [\n",
    "        \"calories\", \"protein_g\", \"carbs_g\", \"fat_g\", \"sodium_mg\",\n",
    "        \"fiber_g\", \"sugar_g\", \"ready_in_min\", \"servings\",\n",
    "        \"vegetarian\", \"vegan\", \"gluten_free\",\n",
    "    ]\n",
    "    fig = plot_feature_distributions(features_12d, feature_names, title=\"12D Nutrition Feature Distributions\")\n",
    "    plt.show()\n",
    "else:\n",
    "    features_12d = None\n",
    "    print(\"No nutrition dataset with sufficient columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_12d is not None and len(features_12d) > 100:\n",
    "    # Standardize for PCA/clustering\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_12d)\n",
    "\n",
    "    # --- PCA ---\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_coords = pca.fit_transform(features_scaled)\n",
    "\n",
    "    print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "    print(f\"  PC1: {pca.explained_variance_ratio_[0]:.1%}\")\n",
    "    print(f\"  PC2: {pca.explained_variance_ratio_[1]:.1%}\")\n",
    "\n",
    "    # --- KMeans clustering ---\n",
    "    n_clusters = 6\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "    # Sample for visualization\n",
    "    MAX_VIS = 5000\n",
    "    if len(pca_coords) > MAX_VIS:\n",
    "        rng = np.random.RandomState(42)\n",
    "        vis_idx = rng.choice(len(pca_coords), size=MAX_VIS, replace=False)\n",
    "    else:\n",
    "        vis_idx = np.arange(len(pca_coords))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # (a) PCA colored by cluster\n",
    "    for cl in range(n_clusters):\n",
    "        mask = cluster_labels[vis_idx] == cl\n",
    "        axes[0].scatter(\n",
    "            pca_coords[vis_idx][mask, 0],\n",
    "            pca_coords[vis_idx][mask, 1],\n",
    "            label=f\"Cluster {cl}\",\n",
    "            color=PALETTE[cl % len(PALETTE)],\n",
    "            alpha=0.5,\n",
    "            s=15,\n",
    "        )\n",
    "    axes[0].set_title(f\"PCA of 12D Nutrition Vectors (K={n_clusters})\", fontsize=13)\n",
    "    axes[0].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.0%} var)\")\n",
    "    axes[0].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.0%} var)\")\n",
    "    axes[0].legend(fontsize=9)\n",
    "\n",
    "    # (b) t-SNE colored by cluster\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    tsne_sample = min(3000, len(features_scaled))\n",
    "    rng2 = np.random.RandomState(42)\n",
    "    tsne_idx = rng2.choice(len(features_scaled), size=tsne_sample, replace=False)\n",
    "\n",
    "    print(f\"Running t-SNE on {tsne_sample:,} nutrition samples...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    tsne_coords = tsne.fit_transform(features_scaled[tsne_idx])\n",
    "\n",
    "    for cl in range(n_clusters):\n",
    "        mask = cluster_labels[tsne_idx] == cl\n",
    "        axes[1].scatter(\n",
    "            tsne_coords[mask, 0],\n",
    "            tsne_coords[mask, 1],\n",
    "            label=f\"Cluster {cl}\",\n",
    "            color=PALETTE[cl % len(PALETTE)],\n",
    "            alpha=0.5,\n",
    "            s=15,\n",
    "        )\n",
    "    axes[1].set_title(f\"t-SNE of 12D Nutrition Vectors (K={n_clusters})\", fontsize=13)\n",
    "    axes[1].set_xlabel(\"t-SNE dim 1\")\n",
    "    axes[1].set_ylabel(\"t-SNE dim 2\")\n",
    "    axes[1].legend(fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(f\"\\nCluster sizes:\")\n",
    "    for cl in range(n_clusters):\n",
    "        mask = cluster_labels == cl\n",
    "        cluster_means = features_12d[mask].mean(axis=0)\n",
    "        print(f\"  Cluster {cl}: {mask.sum():>6,} recipes  \"\n",
    "              f\"| avg cal={cluster_means[0]:.0f}  \"\n",
    "              f\"prot={cluster_means[1]:.1f}g  \"\n",
    "              f\"carbs={cluster_means[2]:.1f}g  \"\n",
    "              f\"fat={cluster_means[3]:.1f}g\")\n",
    "else:\n",
    "    print(\"Skipping PCA/t-SNE/KMeans -- insufficient nutrition data.\")\n",
    "\n",
    "print(\"\\nContent-based analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}