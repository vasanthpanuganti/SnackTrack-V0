{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnackTrack ML --- Dataset Download & Inspection\n",
    "\n",
    "This notebook downloads all **10 Kaggle datasets** used by the SnackTrack ML training pipeline,\n",
    "inspects their structure and data quality, and converts them to **Parquet** format for\n",
    "fast loading in subsequent notebooks.\n",
    "\n",
    "### Datasets overview\n",
    "\n",
    "| Category | Datasets | Purpose |\n",
    "|----------|----------|---------|\n",
    "| **Health/Diet** | `diet_recommendations`, `medical_diet`, `daily_food_nutrition` | Nutritional guidelines, medical diet rules, daily food logs |\n",
    "| **Recipe Corpus** | `foodcom_reviews`, `foodcom_interactions`, `epicurious`, `recipes_64k` | Large-scale recipe data with ratings and nutrition |\n",
    "| **Ingredients** | `recipe_ingredients`, `global_food_nutrition` | Ingredient lists, allergen/nutrient databases |\n",
    "| **Recommendation** | `food_recommendation` | Baseline food + ingredient + rating data |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A [Kaggle API token](https://www.kaggle.com/docs/api) in `~/.kaggle/kaggle.json`\n",
    "- Internet connection for first-time download\n",
    "- ~2 GB free disk space for raw CSV + Parquet copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub pyarrow pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the notebook can find the utils package\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from notebooks.utils.dataset_downloader import DATASETS, download_all, list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all 10 available datasets\n",
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download every dataset (skips those already present on disk)\n",
    "results = download_all()\n",
    "\n",
    "# Quick summary\n",
    "succeeded = sum(1 for v in results.values() if v is not None)\n",
    "print(f\"\\nDownloaded {succeeded}/{len(DATASETS)} datasets successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Inspection\n",
    "\n",
    "For every downloaded dataset we:\n",
    "1. Locate all CSV files in its subdirectory\n",
    "2. Print **shape**, **column names**, **dtypes**, and **null counts**\n",
    "3. Preview the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"../data\") if Path(\"../data\").exists() else Path(\"data\")\n",
    "\n",
    "# Use the resolved DATA_DIR from the downloader as ground truth\n",
    "from notebooks.utils.dataset_downloader import DATA_DIR\n",
    "\n",
    "for name, dest in sorted(results.items()):\n",
    "    if dest is None:\n",
    "        print(f\"=== {name} === SKIPPED (download failed)\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"  Directory: {dest}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    csv_files = sorted(Path(dest).glob(\"*.csv\"))\n",
    "    json_files = sorted(Path(dest).glob(\"*.json\"))\n",
    "    all_files = csv_files + json_files\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"  No CSV/JSON files found.\\n\")\n",
    "        continue\n",
    "\n",
    "    for csv_path in all_files:\n",
    "        try:\n",
    "            if csv_path.suffix == \".json\":\n",
    "                df = pd.read_json(csv_path)\n",
    "            else:\n",
    "                df = pd.read_csv(csv_path, nrows=50_000)  # cap for inspection\n",
    "\n",
    "            print(f\"\\n  File: {csv_path.name}\")\n",
    "            print(f\"  Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "            print(f\"  Columns: {list(df.columns)}\")\n",
    "            print(f\"\\n  Dtypes:\")\n",
    "            print(df.dtypes.to_string().replace(\"\\n\", \"\\n    \"))\n",
    "            print(f\"\\n  Null counts:\")\n",
    "            nulls = df.isnull().sum()\n",
    "            print(nulls[nulls > 0].to_string().replace(\"\\n\", \"\\n    \") or \"    None\")\n",
    "            print(f\"\\n  First 3 rows:\")\n",
    "            display(df.head(3))\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not load {csv_path.name}: {e}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize & Save as Parquet\n",
    "\n",
    "We standardize every dataset to a consistent format:\n",
    "- **Column names** are lowercased with spaces/special chars replaced by underscores\n",
    "- **Parquet** files are written to `data/<dataset_name>.parquet` for fast columnar reads\n",
    "\n",
    "If a dataset contains multiple CSV files, each one is saved as\n",
    "`data/<dataset_name>__<file_stem>.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Lowercase columns, replace spaces/special chars with underscores.\"\"\"\n",
    "    df.columns = [\n",
    "        re.sub(r\"[^a-z0-9]+\", \"_\", col.lower()).strip(\"_\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "saved_files: list[str] = []\n",
    "\n",
    "for name, dest in sorted(results.items()):\n",
    "    if dest is None:\n",
    "        continue\n",
    "\n",
    "    csv_files = sorted(Path(dest).glob(\"*.csv\"))\n",
    "    json_files = sorted(Path(dest).glob(\"*.json\"))\n",
    "    all_files = csv_files + json_files\n",
    "\n",
    "    if not all_files:\n",
    "        continue\n",
    "\n",
    "    for fpath in all_files:\n",
    "        try:\n",
    "            if fpath.suffix == \".json\":\n",
    "                df = pd.read_json(fpath)\n",
    "            else:\n",
    "                df = pd.read_csv(fpath)\n",
    "\n",
    "            df = standardize_columns(df)\n",
    "\n",
    "            # Choose output filename\n",
    "            if len(all_files) == 1:\n",
    "                out_name = f\"{name}.parquet\"\n",
    "            else:\n",
    "                out_name = f\"{name}__{fpath.stem}.parquet\"\n",
    "\n",
    "            out_path = DATA_DIR / out_name\n",
    "            df.to_parquet(out_path, index=False)\n",
    "            saved_files.append(out_name)\n",
    "            print(f\"  Saved {out_name}  ({df.shape[0]:,} rows, {df.shape[1]} cols)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  FAILED {fpath.name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal Parquet files written: {len(saved_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has completed the following steps:\n",
    "\n",
    "1. **Listed** all 10 Kaggle datasets required by the SnackTrack ML pipeline\n",
    "2. **Downloaded** each dataset via `kagglehub` into `data/<dataset_name>/`\n",
    "3. **Inspected** every CSV/JSON file -- shapes, dtypes, nulls, and sample rows\n",
    "4. **Standardized** column names (lowercase + underscores) and saved as **Parquet**\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- **Notebook 01** (`01_eda_and_data_quality.ipynb`): Exploratory data analysis across all datasets\n",
    "- **Notebook 02** (`02_content_based_analysis.ipynb`): Build ingredient/nutrition vectors and test content-based filtering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}