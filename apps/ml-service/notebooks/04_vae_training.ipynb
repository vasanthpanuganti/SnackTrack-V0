{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnackTrack ML — VAE Training\n",
    "\n",
    "**Purpose:** Train the Variational Autoencoder (VAE) that powers SnackTrack's\n",
    "recipe representation learning. The production VAE in `app/recommender/vae.py`\n",
    "currently uses **random placeholder weights** initialized with `np.random.default_rng(42)`.\n",
    "This notebook trains real weights on our recipe corpus and exports them in the\n",
    "exact format the NumPy inference code expects.\n",
    "\n",
    "**Architecture:**\n",
    "- **Encoder:** 12D recipe features -> 32D latent mean (mu) + 32D latent log-variance\n",
    "- **Reparameterization:** z = mu + exp(0.5 * logvar) * epsilon\n",
    "- **Decoder:** 32D latent -> 12D reconstructed features\n",
    "\n",
    "The 12 input features are: `calories`, `protein_g`, `carbs_g`, `fat_g`,\n",
    "`sodium_mg`, `fiber_g`, `sugar_g`, `ready_in_minutes`, `servings`,\n",
    "`is_vegetarian`, `is_vegan`, `is_gluten_free`.\n",
    "\n",
    "**Training strategy:**\n",
    "- Beta-warmup to prevent posterior collapse\n",
    "- Early stopping on validation loss\n",
    "- ReduceLROnPlateau scheduler\n",
    "- Weight export with correct transposition for NumPy inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from notebooks.utils.plot_helpers import (\n",
    "    setup_plot_style,\n",
    "    plot_loss_curves,\n",
    "    plot_latent_space_2d,\n",
    "    plot_feature_distributions,\n",
    ")\n",
    "from notebooks.utils.data_loader import (\n",
    "    load_recipes_from_db,\n",
    "    load_kaggle_dataset,\n",
    "    extract_vae_features,\n",
    ")\n",
    "from notebooks.utils.db_connect import get_connection\n",
    "from notebooks.utils.weight_io import save_vae_weights, load_vae_weights\n",
    "\n",
    "setup_plot_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Data\n",
    "\n",
    "We combine recipe data from all available sources to maximize training set size.\n",
    "The more diverse recipes the VAE sees, the better its latent space will capture\n",
    "the full variety of the recipe landscape.\n",
    "\n",
    "Sources (in priority order):\n",
    "1. **SnackTrack DB** — production recipes\n",
    "2. **Daily Food Nutrition** — Kaggle dataset with detailed nutritional data\n",
    "3. **Food.com** — large recipe corpus from Kaggle\n",
    "4. **Epicurious** — curated recipe dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipe_dfs = []\n",
    "source_counts = {}\n",
    "\n",
    "# --- Source 1: SnackTrack database ---\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    db_recipes = load_recipes_from_db(conn)\n",
    "    conn.close()\n",
    "    if not db_recipes.empty:\n",
    "        all_recipe_dfs.append(db_recipes)\n",
    "        source_counts[\"SnackTrack DB\"] = len(db_recipes)\n",
    "        print(f\"SnackTrack DB:         {len(db_recipes):>7,} recipes\")\n",
    "except Exception as e:\n",
    "    print(f\"SnackTrack DB:         unavailable ({e})\")\n",
    "\n",
    "# --- Source 2: Daily Food Nutrition (Kaggle) ---\n",
    "try:\n",
    "    dfn = load_kaggle_dataset(\"daily_food_nutrition\")\n",
    "    all_recipe_dfs.append(dfn)\n",
    "    source_counts[\"Daily Food Nutrition\"] = len(dfn)\n",
    "    print(f\"Daily Food Nutrition:  {len(dfn):>7,} recipes\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Daily Food Nutrition:  not found (run notebook 00 first)\")\n",
    "\n",
    "# --- Source 3: Food.com recipes (Kaggle) ---\n",
    "try:\n",
    "    foodcom = load_kaggle_dataset(\"food_com_recipes\")\n",
    "    all_recipe_dfs.append(foodcom)\n",
    "    source_counts[\"Food.com\"] = len(foodcom)\n",
    "    print(f\"Food.com:              {len(foodcom):>7,} recipes\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Food.com:              not found (run notebook 00 first)\")\n",
    "\n",
    "# --- Source 4: Epicurious (Kaggle) ---\n",
    "try:\n",
    "    epi = load_kaggle_dataset(\"epicurious\")\n",
    "    all_recipe_dfs.append(epi)\n",
    "    source_counts[\"Epicurious\"] = len(epi)\n",
    "    print(f\"Epicurious:            {len(epi):>7,} recipes\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Epicurious:            not found (run notebook 00 first)\")\n",
    "\n",
    "# --- Fallback: synthetic data if nothing loaded ---\n",
    "if not all_recipe_dfs:\n",
    "    print(\"\\nNo datasets available. Generating synthetic recipe data.\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    n_synthetic = 5000\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        \"calories\": rng.lognormal(5.5, 0.7, n_synthetic).clip(50, 2000),\n",
    "        \"protein_g\": rng.lognormal(2.5, 0.8, n_synthetic).clip(0, 150),\n",
    "        \"carbs_g\": rng.lognormal(3.2, 0.7, n_synthetic).clip(0, 300),\n",
    "        \"fat_g\": rng.lognormal(2.3, 0.9, n_synthetic).clip(0, 150),\n",
    "        \"sodium_mg\": rng.lognormal(5.8, 1.0, n_synthetic).clip(0, 5000),\n",
    "        \"fiber_g\": rng.exponential(3, n_synthetic).clip(0, 50),\n",
    "        \"sugar_g\": rng.lognormal(2.0, 1.0, n_synthetic).clip(0, 100),\n",
    "        \"ready_in_minutes\": rng.lognormal(3.2, 0.6, n_synthetic).clip(5, 300),\n",
    "        \"servings\": rng.choice([1, 2, 4, 6, 8, 12], n_synthetic),\n",
    "        \"diet_labels\": [[] for _ in range(n_synthetic)],\n",
    "    })\n",
    "    # Assign diet labels to a subset\n",
    "    for i in range(n_synthetic):\n",
    "        labels = []\n",
    "        if rng.random() < 0.2:\n",
    "            labels.append(\"vegetarian\")\n",
    "        if rng.random() < 0.08:\n",
    "            labels.append(\"vegan\")\n",
    "        if rng.random() < 0.15:\n",
    "            labels.append(\"gluten free\")\n",
    "        synthetic_df.at[i, \"diet_labels\"] = labels\n",
    "    all_recipe_dfs.append(synthetic_df)\n",
    "    source_counts[\"Synthetic\"] = n_synthetic\n",
    "\n",
    "# --- Combine all sources ---\n",
    "combined_df = pd.concat(all_recipe_dfs, ignore_index=True)\n",
    "print(f\"\\nTotal combined recipes: {len(combined_df):,}\")\n",
    "\n",
    "# --- Extract 12D features ---\n",
    "features_raw = extract_vae_features(combined_df)\n",
    "\n",
    "# Remove rows with any NaN or infinite values\n",
    "valid_mask = np.isfinite(features_raw).all(axis=1)\n",
    "features_raw = features_raw[valid_mask]\n",
    "print(f\"Valid feature vectors:  {len(features_raw):,} (removed {(~valid_mask).sum()} invalid rows)\")\n",
    "print(f\"Feature shape:         {features_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute normalization statistics ---\n",
    "feature_means = features_raw.mean(axis=0)\n",
    "feature_stds = features_raw.std(axis=0)\n",
    "# Prevent division by zero for constant features (e.g., binary flags)\n",
    "feature_stds[feature_stds == 0] = 1.0\n",
    "\n",
    "print(\"Normalization statistics:\")\n",
    "feature_names = [\n",
    "    'calories', 'protein_g', 'carbs_g', 'fat_g', 'sodium_mg', 'fiber_g',\n",
    "    'sugar_g', 'ready_in_minutes', 'servings', 'is_vegetarian', 'is_vegan',\n",
    "    'is_gluten_free',\n",
    "]\n",
    "for name, mu, std in zip(feature_names, feature_means, feature_stds):\n",
    "    print(f\"  {name:<18s}  mean={mu:10.3f}  std={std:10.3f}\")\n",
    "\n",
    "# --- Normalize ---\n",
    "features_normalized = (features_raw - feature_means) / (feature_stds + 1e-8)\n",
    "\n",
    "# --- Train/validation split (80/20) ---\n",
    "rng = np.random.default_rng(42)\n",
    "n_total = len(features_normalized)\n",
    "indices = rng.permutation(n_total)\n",
    "split_idx = int(0.8 * n_total)\n",
    "\n",
    "train_features = features_normalized[indices[:split_idx]]\n",
    "val_features = features_normalized[indices[split_idx:]]\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_features):,} samples\")\n",
    "print(f\"Val set:   {len(val_features):,} samples\")\n",
    "\n",
    "# --- Create PyTorch DataLoaders ---\n",
    "train_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(val_features, dtype=torch.float32)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(train_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(val_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Distributions\n",
    "\n",
    "Before training, we inspect the raw (pre-normalization) feature distributions.\n",
    "This helps us understand the data characteristics:\n",
    "- Continuous nutritional features tend to be right-skewed (log-normal)\n",
    "- Binary diet flags are heavily imbalanced (most recipes are not restricted)\n",
    "- Understanding these distributions helps interpret reconstruction quality later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_feature_distributions(\n",
    "    features_raw,\n",
    "    feature_names,\n",
    "    title=\"Raw Feature Distributions (before normalization)\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define VAE Model\n",
    "\n",
    "The PyTorch model architecture **exactly mirrors** the NumPy inference code in\n",
    "`app/recommender/vae.py`. This is critical: the weight matrices must be\n",
    "compatible after transposition (PyTorch stores `nn.Linear` weights as\n",
    "`(out_features, in_features)`, while NumPy inference uses `x @ W + b` where\n",
    "`W` is `(in_features, out_features)`).\n",
    "\n",
    "**Architecture:**\n",
    "- `encoder_mu`: Linear(12 -> 32) — predicts latent mean\n",
    "- `encoder_logvar`: Linear(12 -> 32) — predicts latent log-variance\n",
    "- `decoder`: Linear(32 -> 12) — reconstructs features from latent code\n",
    "\n",
    "This is intentionally a single-layer encoder/decoder. For recipe features,\n",
    "this linear VAE provides smooth latent spaces suitable for interpolation\n",
    "and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeVAEPyTorch(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for recipe feature embeddings.\n",
    "    \n",
    "    Architecture matches app/recommender/vae.py RecipeVAE exactly.\n",
    "    After training, weights are exported with transposition for NumPy inference.\n",
    "    \"\"\"\n",
    "\n",
    "    LATENT_DIM = 32\n",
    "    FEATURE_DIM = 12\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder_mu = nn.Linear(self.FEATURE_DIM, self.LATENT_DIM)\n",
    "        self.encoder_logvar = nn.Linear(self.FEATURE_DIM, self.LATENT_DIM)\n",
    "        self.decoder = nn.Linear(self.LATENT_DIM, self.FEATURE_DIM)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input features to latent distribution parameters.\"\"\"\n",
    "        return self.encoder_mu(x), self.encoder_logvar(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Sample from latent distribution using reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector back to feature space.\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass: encode, sample, decode.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = RecipeVAEPyTorch().to(device)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:<25s} {str(tuple(param.shape)):<15s} ({param.numel():,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon, original, mu, logvar, beta=1.0):\n",
    "    \"\"\"Compute VAE loss = reconstruction loss + beta * KL divergence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    recon : Tensor\n",
    "        Reconstructed features from the decoder.\n",
    "    original : Tensor\n",
    "        Original input features.\n",
    "    mu : Tensor\n",
    "        Mean of the latent distribution.\n",
    "    logvar : Tensor\n",
    "        Log-variance of the latent distribution.\n",
    "    beta : float\n",
    "        Weight for the KL divergence term (beta-VAE formulation).\n",
    "        During warmup, beta increases from 0 to 1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    total_loss, recon_loss, kl_loss : Tensors\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(recon, original, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "print(\"VAE loss function defined.\")\n",
    "print(\"  - Reconstruction: MSE (sum reduction)\")\n",
    "print(\"  - KL Divergence:  D_KL(q(z|x) || p(z)) with beta weighting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "We train using the beta-VAE formulation with a warmup schedule:\n",
    "\n",
    "- **Beta warmup:** Over the first 30% of epochs, beta increases linearly from 0\n",
    "  to 1. This prevents the common \"posterior collapse\" problem where the KL term\n",
    "  dominates early training and the model learns to ignore the latent code.\n",
    "\n",
    "- **Optimizer:** Adam with lr=1e-3 and weight_decay=1e-5 for mild regularization.\n",
    "\n",
    "- **Scheduler:** ReduceLROnPlateau monitors validation loss with patience=10.\n",
    "\n",
    "- **Early stopping:** Training halts if validation loss does not improve for 20\n",
    "  consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "MAX_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BETA_WARMUP_FRACTION = 0.30  # beta goes 0 -> 1 over first 30% of epochs\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "SCHEDULER_PATIENCE = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=SCHEDULER_PATIENCE, verbose=False\n",
    ")\n",
    "\n",
    "# --- Tracking ---\n",
    "history = {\n",
    "    \"train_total\": [], \"train_recon\": [], \"train_kl\": [],\n",
    "    \"val_total\": [], \"val_recon\": [], \"val_kl\": [],\n",
    "    \"beta\": [], \"lr\": [],\n",
    "}\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "best_state_dict = None\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "beta_warmup_epochs = int(MAX_EPOCHS * BETA_WARMUP_FRACTION)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Max epochs:          {MAX_EPOCHS}\")\n",
    "print(f\"  Learning rate:       {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay:        {WEIGHT_DECAY}\")\n",
    "print(f\"  Beta warmup epochs:  {beta_warmup_epochs}\")\n",
    "print(f\"  Early stop patience: {EARLY_STOP_PATIENCE}\")\n",
    "print(f\"  Scheduler patience:  {SCHEDULER_PATIENCE}\")\n",
    "print()\n",
    "\n",
    "# --- Training loop ---\n",
    "progress = tqdm(range(1, MAX_EPOCHS + 1), desc=\"Training\")\n",
    "\n",
    "for epoch in progress:\n",
    "    # Compute beta for this epoch\n",
    "    if epoch <= beta_warmup_epochs:\n",
    "        beta = epoch / beta_warmup_epochs\n",
    "    else:\n",
    "        beta = 1.0\n",
    "\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_total, train_recon, train_kl = 0.0, 0.0, 0.0\n",
    "    n_train = 0\n",
    "\n",
    "    for (batch,) in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon, mu, logvar = model(batch)\n",
    "        loss, r_loss, kl = vae_loss(recon, batch, mu, logvar, beta=beta)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_total += loss.item()\n",
    "        train_recon += r_loss.item()\n",
    "        train_kl += kl.item()\n",
    "        n_train += len(batch)\n",
    "\n",
    "    # --- Validate ---\n",
    "    model.eval()\n",
    "    val_total, val_recon, val_kl = 0.0, 0.0, 0.0\n",
    "    n_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon, mu, logvar = model(batch)\n",
    "            loss, r_loss, kl = vae_loss(recon, batch, mu, logvar, beta=beta)\n",
    "\n",
    "            val_total += loss.item()\n",
    "            val_recon += r_loss.item()\n",
    "            val_kl += kl.item()\n",
    "            n_val += len(batch)\n",
    "\n",
    "    # --- Normalize by sample count ---\n",
    "    train_total /= n_train\n",
    "    train_recon /= n_train\n",
    "    train_kl /= n_train\n",
    "    val_total /= n_val\n",
    "    val_recon /= n_val\n",
    "    val_kl /= n_val\n",
    "\n",
    "    # --- Record ---\n",
    "    history[\"train_total\"].append(train_total)\n",
    "    history[\"train_recon\"].append(train_recon)\n",
    "    history[\"train_kl\"].append(train_kl)\n",
    "    history[\"val_total\"].append(val_total)\n",
    "    history[\"val_recon\"].append(val_recon)\n",
    "    history[\"val_kl\"].append(val_kl)\n",
    "    history[\"beta\"].append(beta)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    # --- LR scheduler ---\n",
    "    scheduler.step(val_total)\n",
    "\n",
    "    # --- Early stopping check ---\n",
    "    if val_total < best_val_loss:\n",
    "        best_val_loss = val_total\n",
    "        best_epoch = epoch\n",
    "        best_state_dict = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # --- Progress bar update ---\n",
    "    progress.set_postfix({\n",
    "        \"loss\": f\"{val_total:.4f}\",\n",
    "        \"recon\": f\"{val_recon:.4f}\",\n",
    "        \"kl\": f\"{val_kl:.4f}\",\n",
    "        \"beta\": f\"{beta:.2f}\",\n",
    "        \"lr\": f\"{optimizer.param_groups[0]['lr']:.1e}\",\n",
    "        \"best\": f\"{best_epoch}\",\n",
    "    })\n",
    "\n",
    "    if epochs_without_improvement >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}. \"\n",
    "              f\"Best val loss: {best_val_loss:.4f} at epoch {best_epoch}.\")\n",
    "        break\n",
    "\n",
    "# --- Restore best model ---\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    print(f\"\\nRestored best model from epoch {best_epoch} \"\n",
    "          f\"(val loss = {best_val_loss:.4f})\")\n",
    "else:\n",
    "    print(\"\\nTraining completed. Using final epoch weights.\")\n",
    "\n",
    "print(f\"Total epochs run: {len(history['train_total'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Visualization\n",
    "\n",
    "Four panels showing the training dynamics:\n",
    "- **(a) Total loss:** combined reconstruction + beta * KL (train and val)\n",
    "- **(b) Reconstruction loss:** how well the decoder reconstructs features\n",
    "- **(c) KL divergence:** regularization toward the prior N(0,1)\n",
    "- **(d) Beta schedule:** the KL weight warmup curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs_range = range(1, len(history[\"train_total\"]) + 1)\n",
    "\n",
    "# (a) Total loss\n",
    "axes[0, 0].plot(epochs_range, history[\"train_total\"], label=\"Train\", color=\"#4CAF50\", linewidth=1.5)\n",
    "axes[0, 0].plot(epochs_range, history[\"val_total\"], label=\"Val\", color=\"#FF9800\", linewidth=1.5)\n",
    "axes[0, 0].axvline(best_epoch, color=\"#F44336\", linestyle=\"--\", alpha=0.5, label=f\"Best (epoch {best_epoch})\")\n",
    "axes[0, 0].set_title(\"(a) Total Loss\")\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss / sample\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# (b) Reconstruction loss\n",
    "axes[0, 1].plot(epochs_range, history[\"train_recon\"], label=\"Train\", color=\"#4CAF50\", linewidth=1.5)\n",
    "axes[0, 1].plot(epochs_range, history[\"val_recon\"], label=\"Val\", color=\"#FF9800\", linewidth=1.5)\n",
    "axes[0, 1].axvline(best_epoch, color=\"#F44336\", linestyle=\"--\", alpha=0.5)\n",
    "axes[0, 1].set_title(\"(b) Reconstruction Loss (MSE)\")\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"MSE / sample\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# (c) KL divergence\n",
    "axes[1, 0].plot(epochs_range, history[\"train_kl\"], label=\"Train\", color=\"#4CAF50\", linewidth=1.5)\n",
    "axes[1, 0].plot(epochs_range, history[\"val_kl\"], label=\"Val\", color=\"#FF9800\", linewidth=1.5)\n",
    "axes[1, 0].axvline(best_epoch, color=\"#F44336\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1, 0].set_title(\"(c) KL Divergence\")\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"KL / sample\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# (d) Beta schedule + learning rate\n",
    "ax_beta = axes[1, 1]\n",
    "ax_lr = ax_beta.twinx()\n",
    "\n",
    "line1, = ax_beta.plot(epochs_range, history[\"beta\"], label=\"Beta\", color=\"#9C27B0\", linewidth=2)\n",
    "line2, = ax_lr.plot(epochs_range, history[\"lr\"], label=\"Learning Rate\", color=\"#2196F3\",\n",
    "                    linewidth=1.5, linestyle=\"--\")\n",
    "\n",
    "ax_beta.set_title(\"(d) Beta Schedule & Learning Rate\")\n",
    "ax_beta.set_xlabel(\"Epoch\")\n",
    "ax_beta.set_ylabel(\"Beta\", color=\"#9C27B0\")\n",
    "ax_lr.set_ylabel(\"Learning Rate\", color=\"#2196F3\")\n",
    "ax_beta.set_ylim(-0.05, 1.1)\n",
    "ax_lr.set_yscale(\"log\")\n",
    "ax_beta.legend(handles=[line1, line2], loc=\"center right\")\n",
    "ax_beta.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss:   {history['train_total'][-1]:.4f}\")\n",
    "print(f\"Best validation loss:  {best_val_loss:.4f} (epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Latent Space Visualization\n",
    "\n",
    "We encode all recipes into the 32D latent space, then project down to 2D with\n",
    "t-SNE. If diet labels are available, we color-code points by diet type to see\n",
    "whether the VAE has learned to separate different recipe categories in its\n",
    "latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all recipes\n",
    "model.eval()\n",
    "all_features_tensor = torch.tensor(features_normalized, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_mu, _ = model.encode(all_features_tensor)\n",
    "    all_embeddings = all_mu.cpu().numpy()\n",
    "\n",
    "print(f\"Encoded {len(all_embeddings)} recipes to {all_embeddings.shape[1]}D latent space.\")\n",
    "\n",
    "# Build labels from diet flags (use the raw features columns 9, 10, 11)\n",
    "labels = []\n",
    "for i in range(len(features_raw)):\n",
    "    if features_raw[i, 10] > 0.5:       # is_vegan\n",
    "        labels.append(\"Vegan\")\n",
    "    elif features_raw[i, 9] > 0.5:       # is_vegetarian\n",
    "        labels.append(\"Vegetarian\")\n",
    "    elif features_raw[i, 11] > 0.5:      # is_gluten_free\n",
    "        labels.append(\"Gluten-Free\")\n",
    "    else:\n",
    "        labels.append(\"Standard\")\n",
    "\n",
    "# Subsample for t-SNE if dataset is large (t-SNE is O(n^2))\n",
    "max_tsne = 5000\n",
    "if len(all_embeddings) > max_tsne:\n",
    "    rng = np.random.default_rng(42)\n",
    "    sample_idx = rng.choice(len(all_embeddings), max_tsne, replace=False)\n",
    "    plot_embeddings = all_embeddings[sample_idx]\n",
    "    plot_labels = [labels[i] for i in sample_idx]\n",
    "    print(f\"Subsampled to {max_tsne} points for t-SNE.\")\n",
    "else:\n",
    "    plot_embeddings = all_embeddings\n",
    "    plot_labels = labels\n",
    "\n",
    "fig = plot_latent_space_2d(\n",
    "    plot_embeddings,\n",
    "    labels=plot_labels,\n",
    "    title=\"VAE Latent Space — Recipes Colored by Diet Type\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print label distribution\n",
    "from collections import Counter\n",
    "label_counts = Counter(labels)\n",
    "print(\"\\nLabel distribution:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"  {label:<15s}: {count:>6,} ({count / len(labels):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reconstruction Quality\n",
    "\n",
    "We evaluate how well the VAE reconstructs recipe features by comparing original\n",
    "and decoded features for a sample of 5 recipes. We also compute per-feature MSE\n",
    "across the entire validation set to identify which features are hardest to\n",
    "reconstruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-feature MSE on validation set ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_all = val_tensor.to(device)\n",
    "    recon_all, _, _ = model(val_all)\n",
    "    recon_np = recon_all.cpu().numpy()\n",
    "    val_np = val_all.cpu().numpy()\n",
    "\n",
    "per_feature_mse = ((recon_np - val_np) ** 2).mean(axis=0)\n",
    "\n",
    "print(\"Per-feature reconstruction MSE (normalized space):\")\n",
    "print(f\"{'Feature':<20s} {'MSE':>10s}\")\n",
    "print(\"-\" * 32)\n",
    "for name, mse in zip(feature_names, per_feature_mse):\n",
    "    print(f\"{name:<20s} {mse:>10.6f}\")\n",
    "print(f\"{'TOTAL (avg)':<20s} {per_feature_mse.mean():>10.6f}\")\n",
    "\n",
    "# --- Sample comparison table (de-normalized to original scale) ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample Reconstruction Comparison (original scale)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_samples = 5\n",
    "sample_indices = np.linspace(0, len(val_np) - 1, n_samples, dtype=int)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    original_norm = val_np[idx]\n",
    "    reconstructed_norm = recon_np[idx]\n",
    "\n",
    "    # De-normalize back to original scale\n",
    "    original_raw = original_norm * (feature_stds + 1e-8) + feature_means\n",
    "    reconstructed_raw = reconstructed_norm * (feature_stds + 1e-8) + feature_means\n",
    "\n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"  {'Feature':<20s} {'Original':>12s} {'Reconstructed':>14s} {'Error':>10s}\")\n",
    "    print(f\"  {'-'*20} {'-'*12} {'-'*14} {'-'*10}\")\n",
    "    for name, orig, rec in zip(feature_names, original_raw, reconstructed_raw):\n",
    "        err = abs(orig - rec)\n",
    "        print(f\"  {name:<20s} {orig:>12.2f} {rec:>14.2f} {err:>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Weights\n",
    "\n",
    "Export the trained PyTorch weights to the format expected by the NumPy-based\n",
    "production `RecipeVAE` in `app/recommender/vae.py`.\n",
    "\n",
    "**Critical: Weight Transposition**\n",
    "\n",
    "PyTorch `nn.Linear` stores weights as `(out_features, in_features)` and computes\n",
    "`y = x @ W.T + b`. The NumPy inference code computes `y = x @ W + b` where `W`\n",
    "is `(in_features, out_features)`. Therefore we must **transpose** all weight\n",
    "matrices during export:\n",
    "\n",
    "| Weight | PyTorch shape | Exported (NumPy) shape |\n",
    "|--------|--------------|------------------------|\n",
    "| encoder_mu_w | (32, 12) | (12, 32) |\n",
    "| encoder_logvar_w | (32, 12) | (12, 32) |\n",
    "| decoder_w | (12, 32) | (32, 12) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build weight dict with CORRECT transposition for NumPy inference\n",
    "weights = {\n",
    "    # Encoder mu: PyTorch (32,12) -> NumPy (12,32)\n",
    "    'encoder_mu_w': model.encoder_mu.weight.detach().cpu().numpy().T,\n",
    "    'encoder_mu_b': model.encoder_mu.bias.detach().cpu().numpy(),\n",
    "\n",
    "    # Encoder logvar: PyTorch (32,12) -> NumPy (12,32)\n",
    "    'encoder_logvar_w': model.encoder_logvar.weight.detach().cpu().numpy().T,\n",
    "    'encoder_logvar_b': model.encoder_logvar.bias.detach().cpu().numpy(),\n",
    "\n",
    "    # Decoder: PyTorch (12,32) -> NumPy (32,12)\n",
    "    'decoder_w': model.decoder.weight.detach().cpu().numpy().T,\n",
    "    'decoder_b': model.decoder.bias.detach().cpu().numpy(),\n",
    "\n",
    "    # Normalization statistics (computed from training data)\n",
    "    'feature_means': feature_means,\n",
    "    'feature_stds': feature_stds,\n",
    "}\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"Exported weight shapes:\")\n",
    "for key, arr in weights.items():\n",
    "    print(f\"  {key:<25s} {str(arr.shape):<15s} dtype={arr.dtype}\")\n",
    "\n",
    "# Save to disk\n",
    "save_path = save_vae_weights(weights)\n",
    "print(f\"\\nWeights saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verification\n",
    "\n",
    "We verify that the exported weights produce **identical results** when used in\n",
    "the NumPy inference path (which mirrors `RecipeVAE.encode()` in production).\n",
    "\n",
    "Specifically, we:\n",
    "1. Load the saved `.npz` weights\n",
    "2. Replicate the NumPy encode logic (`normalized @ W + b`)\n",
    "3. Compare against the PyTorch model's deterministic output (mu)\n",
    "4. Assert that results match within numerical tolerance (atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load exported weights ---\n",
    "loaded_weights = load_vae_weights()\n",
    "print(\"Loaded weights from disk.\")\n",
    "\n",
    "# --- Replicate NumPy encode logic (mirrors RecipeVAE.encode in production) ---\n",
    "def numpy_encode(features_raw, w):\n",
    "    \"\"\"NumPy inference path matching app/recommender/vae.py RecipeVAE.encode().\"\"\"\n",
    "    normalized = (features_raw - w['feature_means']) / (w['feature_stds'] + 1e-8)\n",
    "    mu = normalized @ w['encoder_mu_w'] + w['encoder_mu_b']\n",
    "    logvar = normalized @ w['encoder_logvar_w'] + w['encoder_logvar_b']\n",
    "    return mu, logvar\n",
    "\n",
    "# --- Select test samples ---\n",
    "n_test = 100\n",
    "test_raw = features_raw[:n_test]  # un-normalized raw features\n",
    "\n",
    "# --- NumPy path ---\n",
    "np_mu, np_logvar = numpy_encode(test_raw, loaded_weights)\n",
    "\n",
    "# --- PyTorch path (normalize the same way, then encode) ---\n",
    "model.eval()\n",
    "test_normalized = (test_raw - feature_means) / (feature_stds + 1e-8)\n",
    "test_tensor = torch.tensor(test_normalized, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pt_mu, pt_logvar = model.encode(test_tensor)\n",
    "    pt_mu = pt_mu.cpu().numpy()\n",
    "    pt_logvar = pt_logvar.cpu().numpy()\n",
    "\n",
    "# --- Compare ---\n",
    "mu_max_diff = np.max(np.abs(np_mu - pt_mu))\n",
    "logvar_max_diff = np.max(np.abs(np_logvar - pt_logvar))\n",
    "\n",
    "print(f\"\\nVerification results ({n_test} test samples):\")\n",
    "print(f\"  mu max absolute difference:     {mu_max_diff:.2e}\")\n",
    "print(f\"  logvar max absolute difference:  {logvar_max_diff:.2e}\")\n",
    "\n",
    "# --- Assert ---\n",
    "TOLERANCE = 1e-5\n",
    "assert np.allclose(np_mu, pt_mu, atol=TOLERANCE), (\n",
    "    f\"mu mismatch! Max diff: {mu_max_diff:.2e} > tolerance {TOLERANCE:.2e}\"\n",
    ")\n",
    "assert np.allclose(np_logvar, pt_logvar, atol=TOLERANCE), (\n",
    "    f\"logvar mismatch! Max diff: {logvar_max_diff:.2e} > tolerance {TOLERANCE:.2e}\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"  Verification PASSED  (tolerance = {TOLERANCE:.0e})\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nThe exported weights produce identical results in both\")\n",
    "print(f\"the PyTorch training model and the NumPy inference path.\")\n",
    "print(f\"The weights at '{save_path}' are ready for production use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}