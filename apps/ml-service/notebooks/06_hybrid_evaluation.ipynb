{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnackTrack ML --- Hybrid Recommendation System Evaluation\n",
    "\n",
    "This notebook provides a comprehensive evaluation of the SnackTrack **hybrid recommendation system**,\n",
    "which blends five distinct recommendation strategies:\n",
    "\n",
    "| Model | Approach | When it shines |\n",
    "|-------|----------|----------------|\n",
    "| **Content-Based** | pgvector cosine similarity on recipe embeddings | Users with clear taste profiles |\n",
    "| **Collaborative** | User-user similarity from interaction patterns | Users with overlapping preferences |\n",
    "| **Knowledge-Based** | Expert nutritional guidelines and dietary constraints | Cold-start, health-focused users |\n",
    "| **VAE** | Variational Autoencoder latent space similarity | Sparse data, exploration |\n",
    "| **RNN** | Sequential meal pattern prediction (GRU) | Users with temporal eating patterns |\n",
    "\n",
    "### Evaluation focus\n",
    "\n",
    "1. **Per-model performance**: How well does each individual model rank relevant items?\n",
    "2. **Hybrid blending**: Does the weighted combination outperform any single model?\n",
    "3. **Before vs After**: Impact of replacing random placeholder weights with trained weights.\n",
    "4. **Weight sensitivity**: How robust is the hybrid to changes in blending weights?\n",
    "5. **Cold-start analysis**: Performance across user maturity segments.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- **Precision@K**: Fraction of recommended items that are relevant\n",
    "- **Recall@K**: Fraction of relevant items that are recommended\n",
    "- **NDCG@K**: Normalized Discounted Cumulative Gain (rank-aware)\n",
    "- **MRR**: Mean Reciprocal Rank of the first relevant item\n",
    "- **Coverage**: Fraction of the item catalogue recommended at least once\n",
    "- **Diversity**: Average pairwise cosine distance among recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Path setup\n",
    "# ---------------------------------------------------------------------------\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from notebooks.utils.plot_helpers import (\n",
    "    setup_plot_style, SNACKTRACK_COLORS, PALETTE,\n",
    ")\n",
    "from notebooks.utils.data_loader import (\n",
    "    load_kaggle_dataset, extract_vae_features, _encode_time_features,\n",
    ")\n",
    "from notebooks.utils.weight_io import (\n",
    "    load_vae_weights, load_rnn_weights,\n",
    "    VAE_WEIGHT_SHAPES, RNN_WEIGHT_SHAPES,\n",
    ")\n",
    "\n",
    "setup_plot_style()\n",
    "print(f\"NumPy {np.__version__} | Pandas {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Weights\n",
    "\n",
    "We load the VAE and RNN weights produced by notebooks 04 and 05 respectively.\n",
    "If the weight files are not found (e.g., the training notebooks haven't been run yet),\n",
    "we fall back to random weights and print a prominent warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Load VAE weights\n",
    "# ---------------------------------------------------------------------------\n",
    "vae_weights = None\n",
    "vae_trained = False\n",
    "\n",
    "try:\n",
    "    vae_weights = load_vae_weights()\n",
    "    vae_trained = True\n",
    "    print(\"VAE weights loaded successfully:\")\n",
    "    for key, arr in vae_weights.items():\n",
    "        print(f\"  {key:<20} {arr.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"WARNING: VAE weights not found! Using random weights.\")\n",
    "    print(\"  Run notebook 04 (VAE training) first for meaningful results.\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    vae_weights = {}\n",
    "    for key, shape in VAE_WEIGHT_SHAPES.items():\n",
    "        if \"b\" in key or \"mean\" in key:\n",
    "            vae_weights[key] = np.zeros(shape)\n",
    "        elif \"std\" in key:\n",
    "            vae_weights[key] = np.ones(shape)\n",
    "        else:\n",
    "            vae_weights[key] = rng.standard_normal(shape) * 0.1\n",
    "\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Load RNN weights\n",
    "# ---------------------------------------------------------------------------\n",
    "rnn_weights = None\n",
    "rnn_trained = False\n",
    "\n",
    "try:\n",
    "    rnn_weights = load_rnn_weights()\n",
    "    rnn_trained = True\n",
    "    print(\"RNN weights loaded successfully:\")\n",
    "    for key, arr in rnn_weights.items():\n",
    "        print(f\"  {key:<5} {arr.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"WARNING: RNN weights not found! Using random weights.\")\n",
    "    print(\"  Run notebook 05 (RNN training) first for meaningful results.\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    rnn_weights = {}\n",
    "    for key, shape in RNN_WEIGHT_SHAPES.items():\n",
    "        if key.startswith(\"b\"):\n",
    "            rnn_weights[key] = np.zeros(shape)\n",
    "        else:\n",
    "            rnn_weights[key] = rng.standard_normal(shape) * 0.1\n",
    "\n",
    "print()\n",
    "print(f\"VAE: {'TRAINED' if vae_trained else 'RANDOM'} weights\")\n",
    "print(f\"RNN: {'TRAINED' if rnn_trained else 'RANDOM'} weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Evaluation Data\n",
    "\n",
    "We load the Food.com interactions dataset and create a **temporal split**:\n",
    "- **Training set**: First 80% of each user's interactions (chronologically).\n",
    "- **Test set**: Last 20% of each user's interactions.\n",
    "\n",
    "This simulates real-world conditions where we train on past behaviour\n",
    "and evaluate on future interactions.\n",
    "\n",
    "We also build user profiles from the training set: a preference vector\n",
    "computed as the weighted average of interacted recipe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Load interaction data\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    interactions_df = load_kaggle_dataset(\"foodcom_interactions\")\n",
    "    print(f\"  Food.com interactions: {len(interactions_df):,} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(\"  Food.com interactions not found. Generating synthetic data.\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    n_users, n_recipes = 300, 500\n",
    "    n_interactions = 15000\n",
    "    interactions_df = pd.DataFrame({\n",
    "        \"user_id\": rng.integers(0, n_users, n_interactions),\n",
    "        \"recipe_id\": rng.integers(0, n_recipes, n_interactions),\n",
    "        \"rating\": rng.integers(1, 6, n_interactions),\n",
    "        \"date\": pd.date_range(\"2020-01-01\", periods=n_interactions, freq=\"30min\"),\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Identify columns\n",
    "# ---------------------------------------------------------------------------\n",
    "user_col = next((c for c in [\"user_id\", \"author_id\", \"contributor_id\"]\n",
    "                 if c in interactions_df.columns), None)\n",
    "recipe_col = next((c for c in [\"recipe_id\", \"id\"]\n",
    "                   if c in interactions_df.columns), None)\n",
    "rating_col = next((c for c in [\"rating\", \"score\", \"interaction_value\"]\n",
    "                   if c in interactions_df.columns), None)\n",
    "date_col = None\n",
    "for col in [\"date\", \"submitted\", \"created_at\", \"timestamp\"]:\n",
    "    if col in interactions_df.columns:\n",
    "        date_col = col\n",
    "        break\n",
    "\n",
    "if date_col:\n",
    "    interactions_df[\"timestamp\"] = pd.to_datetime(interactions_df[date_col], errors=\"coerce\")\n",
    "else:\n",
    "    interactions_df[\"timestamp\"] = pd.date_range(\n",
    "        start=\"2020-01-01\", periods=len(interactions_df), freq=\"h\"\n",
    "    )\n",
    "\n",
    "interactions_df = interactions_df.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "print(f\"  User col: {user_col}, Recipe col: {recipe_col}, Rating col: {rating_col}\")\n",
    "print(f\"  Total interactions: {len(interactions_df):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Build recipe embeddings (feature-based proxy)\n",
    "# ---------------------------------------------------------------------------\n",
    "all_recipe_ids = interactions_df[recipe_col].unique()\n",
    "\n",
    "# Try to load recipe nutritional data\n",
    "recipe_features = {}\n",
    "try:\n",
    "    recipes_df = load_kaggle_dataset(\"foodcom_reviews\")\n",
    "    for _, row in recipes_df.iterrows():\n",
    "        rid = row.get(recipe_col) or row.get(\"id\") or row.get(\"recipe_id\")\n",
    "        if rid is not None:\n",
    "            emb = np.zeros(32, dtype=np.float64)\n",
    "            emb[0] = (row.get(\"calories\") or 0) / 1000.0\n",
    "            emb[1] = (row.get(\"protein\") or row.get(\"protein_g\") or 0) / 100.0\n",
    "            emb[2] = (row.get(\"carbohydrate\") or row.get(\"carbs_g\") or 0) / 200.0\n",
    "            emb[3] = (row.get(\"total_fat\") or row.get(\"fat_g\") or 0) / 100.0\n",
    "            emb[4] = (row.get(\"sodium\") or row.get(\"sodium_mg\") or 0) / 2300.0\n",
    "            emb[5] = (row.get(\"fiber\") or row.get(\"fiber_g\") or 0) / 30.0\n",
    "            emb[6] = (row.get(\"sugar\") or row.get(\"sugar_g\") or 0) / 50.0\n",
    "            recipe_features[rid] = emb\n",
    "    print(f\"  Loaded {len(recipe_features):,} recipe feature vectors\")\n",
    "except (FileNotFoundError, Exception) as e:\n",
    "    print(f\"  Recipe features not available ({e}). Using random embeddings.\")\n",
    "\n",
    "# Fill missing with random\n",
    "rng_emb = np.random.default_rng(123)\n",
    "for rid in all_recipe_ids:\n",
    "    if rid not in recipe_features:\n",
    "        recipe_features[rid] = rng_emb.standard_normal(32) * 0.1\n",
    "\n",
    "print(f\"  Total recipe embeddings: {len(recipe_features):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Temporal train/test split (last 20% of each user's interactions)\n",
    "# ---------------------------------------------------------------------------\n",
    "TEST_FRACTION = 0.2\n",
    "MIN_INTERACTIONS = 5  # need at least 5 interactions to evaluate\n",
    "\n",
    "train_data = []  # list of (user_id, recipe_id, rating)\n",
    "test_data = {}   # user_id -> set of relevant recipe_ids\n",
    "user_profiles = {}  # user_id -> preference vector\n",
    "user_interaction_counts = {}  # user_id -> count\n",
    "\n",
    "for uid, group in interactions_df.sort_values(\"timestamp\").groupby(user_col):\n",
    "    if len(group) < MIN_INTERACTIONS:\n",
    "        continue\n",
    "\n",
    "    split_idx = int(len(group) * (1 - TEST_FRACTION))\n",
    "    train_group = group.iloc[:split_idx]\n",
    "    test_group = group.iloc[split_idx:]\n",
    "\n",
    "    # Training data\n",
    "    for _, row in train_group.iterrows():\n",
    "        rating = float(row[rating_col]) if rating_col else 3.0\n",
    "        train_data.append((uid, row[recipe_col], rating))\n",
    "\n",
    "    # Test set: recipes the user interacted with in the test period\n",
    "    # Only consider positive interactions (rating >= 3) as \"relevant\"\n",
    "    if rating_col:\n",
    "        relevant = set(test_group[test_group[rating_col] >= 3][recipe_col].values)\n",
    "    else:\n",
    "        relevant = set(test_group[recipe_col].values)\n",
    "\n",
    "    if len(relevant) > 0:\n",
    "        test_data[uid] = relevant\n",
    "\n",
    "    # Build user preference vector from training interactions\n",
    "    vectors = []\n",
    "    weights = []\n",
    "    for _, row in train_group.iterrows():\n",
    "        rid = row[recipe_col]\n",
    "        if rid in recipe_features:\n",
    "            rating = float(row[rating_col]) if rating_col else 3.0\n",
    "            vectors.append(recipe_features[rid])\n",
    "            weights.append(rating)\n",
    "\n",
    "    if vectors:\n",
    "        vectors_arr = np.array(vectors)\n",
    "        weights_arr = np.array(weights).reshape(-1, 1)\n",
    "        pref_vec = (vectors_arr * weights_arr).sum(axis=0) / (weights_arr.sum() + 1e-8)\n",
    "        norm = np.linalg.norm(pref_vec)\n",
    "        if norm > 0:\n",
    "            pref_vec = pref_vec / norm\n",
    "        user_profiles[uid] = pref_vec\n",
    "\n",
    "    user_interaction_counts[uid] = len(train_group)\n",
    "\n",
    "print(f\"\\nEvaluation dataset:\")\n",
    "print(f\"  Training interactions: {len(train_data):,}\")\n",
    "print(f\"  Test users:           {len(test_data):,}\")\n",
    "print(f\"  Users with profiles:  {len(user_profiles):,}\")\n",
    "print(f\"  Unique recipes:       {len(all_recipe_ids):,}\")\n",
    "print(f\"  Avg test items/user:  {np.mean([len(v) for v in test_data.values()]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics\n",
    "\n",
    "Standard information retrieval metrics adapted for top-K recommendation:\n",
    "\n",
    "- **Precision@K**: Of the top K items recommended, how many are relevant?\n",
    "- **Recall@K**: Of all relevant items, how many appear in the top K?\n",
    "- **NDCG@K**: How well are relevant items ranked? Earlier positions get more credit.\n",
    "- **MRR**: How quickly does the first relevant item appear?\n",
    "- **Coverage**: What fraction of the item catalogue gets recommended?\n",
    "- **Diversity**: How different are the recommended items from each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended, relevant, k):\n",
    "    \"\"\"Precision@K: fraction of top-K recommendations that are relevant.\n",
    "\n",
    "    Args:\n",
    "        recommended: ordered list of recommended item IDs\n",
    "        relevant: set of relevant (ground-truth) item IDs\n",
    "        k: cutoff\n",
    "\n",
    "    Returns:\n",
    "        float in [0, 1]\n",
    "    \"\"\"\n",
    "    if k == 0 or not recommended:\n",
    "        return 0.0\n",
    "    top_k = recommended[:k]\n",
    "    hits = len(set(top_k) & set(relevant))\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    \"\"\"Recall@K: fraction of relevant items that appear in top-K.\n",
    "\n",
    "    Args:\n",
    "        recommended: ordered list of recommended item IDs\n",
    "        relevant: set of relevant (ground-truth) item IDs\n",
    "        k: cutoff\n",
    "\n",
    "    Returns:\n",
    "        float in [0, 1]\n",
    "    \"\"\"\n",
    "    if not relevant or not recommended:\n",
    "        return 0.0\n",
    "    top_k = recommended[:k]\n",
    "    hits = len(set(top_k) & set(relevant))\n",
    "    return hits / len(relevant)\n",
    "\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain at K.\n",
    "\n",
    "    Measures ranking quality: relevant items ranked higher contribute more.\n",
    "\n",
    "    Args:\n",
    "        recommended: ordered list of recommended item IDs\n",
    "        relevant: set of relevant (ground-truth) item IDs\n",
    "        k: cutoff\n",
    "\n",
    "    Returns:\n",
    "        float in [0, 1]\n",
    "    \"\"\"\n",
    "    if not relevant or not recommended:\n",
    "        return 0.0\n",
    "\n",
    "    top_k = recommended[:k]\n",
    "\n",
    "    # DCG: sum of 1/log2(rank+1) for each relevant item in top-K\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(top_k):\n",
    "        if item in relevant:\n",
    "            dcg += 1.0 / np.log2(i + 2)  # +2 because rank starts at 1\n",
    "\n",
    "    # Ideal DCG: all relevant items ranked first\n",
    "    ideal_hits = min(len(relevant), k)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(recommended, relevant):\n",
    "    \"\"\"Mean Reciprocal Rank: 1 / rank of first relevant item.\n",
    "\n",
    "    Args:\n",
    "        recommended: ordered list of recommended item IDs\n",
    "        relevant: set of relevant (ground-truth) item IDs\n",
    "\n",
    "    Returns:\n",
    "        float in [0, 1]\n",
    "    \"\"\"\n",
    "    if not relevant or not recommended:\n",
    "        return 0.0\n",
    "    for i, item in enumerate(recommended):\n",
    "        if item in relevant:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def coverage(all_recommendations, total_items):\n",
    "    \"\"\"Catalogue coverage: fraction of items recommended at least once.\n",
    "\n",
    "    Args:\n",
    "        all_recommendations: list of recommendation lists (one per user)\n",
    "        total_items: total number of items in the catalogue\n",
    "\n",
    "    Returns:\n",
    "        float in [0, 1]\n",
    "    \"\"\"\n",
    "    if total_items == 0:\n",
    "        return 0.0\n",
    "    recommended_items = set()\n",
    "    for recs in all_recommendations:\n",
    "        recommended_items.update(recs)\n",
    "    return len(recommended_items) / total_items\n",
    "\n",
    "\n",
    "def diversity(recommendations, embeddings):\n",
    "    \"\"\"Intra-list diversity: average pairwise cosine distance among recommendations.\n",
    "\n",
    "    Higher is more diverse.\n",
    "\n",
    "    Args:\n",
    "        recommendations: list of recommended item IDs\n",
    "        embeddings: dict mapping item_id -> embedding vector\n",
    "\n",
    "    Returns:\n",
    "        float in [0, 2] (cosine distance range)\n",
    "    \"\"\"\n",
    "    if len(recommendations) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    vecs = []\n",
    "    for rid in recommendations:\n",
    "        if rid in embeddings:\n",
    "            vecs.append(embeddings[rid])\n",
    "\n",
    "    if len(vecs) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    vecs = np.array(vecs)\n",
    "    total_dist = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(vecs)):\n",
    "        for j in range(i + 1, len(vecs)):\n",
    "            norm_i = np.linalg.norm(vecs[i])\n",
    "            norm_j = np.linalg.norm(vecs[j])\n",
    "            if norm_i > 0 and norm_j > 0:\n",
    "                cos_sim = np.dot(vecs[i], vecs[j]) / (norm_i * norm_j)\n",
    "                total_dist += 1.0 - cos_sim  # cosine distance\n",
    "            count += 1\n",
    "\n",
    "    return total_dist / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"Metric functions defined:\")\n",
    "print(\"  precision_at_k, recall_at_k, ndcg_at_k,\")\n",
    "print(\"  mean_reciprocal_rank, coverage, diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Per-Model Evaluation\n",
    "\n",
    "We evaluate each of the five recommendation models independently on the test set.\n",
    "Since we are running offline (without a database), we implement lightweight\n",
    "NumPy-based versions of each model that replicate the production scoring logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Model implementations (offline NumPy versions)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def content_based_recommend(user_id, user_profiles, recipe_features, top_n=10, exclude=None):\n",
    "    \"\"\"Content-based: cosine similarity between user profile and recipe embeddings.\"\"\"\n",
    "    if user_id not in user_profiles:\n",
    "        return []\n",
    "    pref = user_profiles[user_id]\n",
    "    pref_norm = np.linalg.norm(pref)\n",
    "    if pref_norm == 0:\n",
    "        return []\n",
    "\n",
    "    exclude = set(exclude or [])\n",
    "    scored = []\n",
    "    for rid, emb in recipe_features.items():\n",
    "        if rid in exclude:\n",
    "            continue\n",
    "        emb_norm = np.linalg.norm(emb)\n",
    "        if emb_norm > 0:\n",
    "            sim = float(np.dot(pref, emb) / (pref_norm * emb_norm))\n",
    "        else:\n",
    "            sim = 0.0\n",
    "        scored.append((rid, sim))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [rid for rid, _ in scored[:top_n]]\n",
    "\n",
    "\n",
    "def collaborative_recommend(user_id, train_data, user_interaction_counts,\n",
    "                            recipe_features, top_n=10, exclude=None):\n",
    "    \"\"\"Collaborative filtering: user-user similarity from interaction overlap.\"\"\"\n",
    "    exclude = set(exclude or [])\n",
    "\n",
    "    # Build user-item matrix (sparse representation)\n",
    "    user_items = {}  # user_id -> {recipe_id: rating}\n",
    "    for uid, rid, rating in train_data:\n",
    "        if uid not in user_items:\n",
    "            user_items[uid] = {}\n",
    "        user_items[uid][rid] = rating\n",
    "\n",
    "    if user_id not in user_items:\n",
    "        return []\n",
    "\n",
    "    target_items = user_items[user_id]\n",
    "\n",
    "    # Find users with overlapping interactions\n",
    "    similarities = []\n",
    "    for other_uid, other_items in user_items.items():\n",
    "        if other_uid == user_id:\n",
    "            continue\n",
    "        overlap = set(target_items.keys()) & set(other_items.keys())\n",
    "        if len(overlap) < 2:\n",
    "            continue\n",
    "\n",
    "        # Cosine similarity on overlapping ratings\n",
    "        v1 = np.array([target_items[r] for r in overlap])\n",
    "        v2 = np.array([other_items[r] for r in overlap])\n",
    "        n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "        if n1 > 0 and n2 > 0:\n",
    "            sim = float(np.dot(v1, v2) / (n1 * n2))\n",
    "            similarities.append((other_uid, sim))\n",
    "\n",
    "    if not similarities:\n",
    "        return []\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_neighbors = similarities[:20]\n",
    "\n",
    "    # Aggregate neighbor ratings\n",
    "    candidate_scores = {}\n",
    "    for neighbor_uid, sim in top_neighbors:\n",
    "        if sim <= 0:\n",
    "            continue\n",
    "        for rid, rating in user_items[neighbor_uid].items():\n",
    "            if rid in target_items or rid in exclude:\n",
    "                continue\n",
    "            candidate_scores[rid] = candidate_scores.get(rid, 0.0) + sim * rating\n",
    "\n",
    "    scored = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [rid for rid, _ in scored[:top_n]]\n",
    "\n",
    "\n",
    "def knowledge_based_recommend(user_id, recipe_features, top_n=10, exclude=None):\n",
    "    \"\"\"Knowledge-based: score by nutritional balance heuristics.\"\"\"\n",
    "    exclude = set(exclude or [])\n",
    "\n",
    "    scored = []\n",
    "    for rid, emb in recipe_features.items():\n",
    "        if rid in exclude:\n",
    "            continue\n",
    "        # Nutritional balance score: penalise extreme values\n",
    "        # emb[0] = cal/1000, emb[1] = protein/100, emb[2] = carbs/200, emb[3] = fat/100\n",
    "        cal = emb[0] * 1000\n",
    "        protein = emb[1] * 100\n",
    "        carbs = emb[2] * 200\n",
    "        fat = emb[3] * 100\n",
    "\n",
    "        # Target: 500 cal meal, 25g protein, 60g carbs, 20g fat\n",
    "        cal_score = max(0, 1.0 - abs(cal - 500) / 500 * 0.5)\n",
    "        protein_score = max(0, 1.0 - abs(protein - 25) / 25 * 0.5)\n",
    "        carb_score = max(0, 1.0 - abs(carbs - 60) / 60 * 0.5)\n",
    "        fat_score = max(0, 1.0 - abs(fat - 20) / 20 * 0.5)\n",
    "\n",
    "        score = 0.3 * cal_score + 0.25 * protein_score + 0.25 * carb_score + 0.2 * fat_score\n",
    "        scored.append((rid, score))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [rid for rid, _ in scored[:top_n]]\n",
    "\n",
    "\n",
    "def vae_recommend(user_id, user_profiles, recipe_features, vae_w, top_n=10, exclude=None):\n",
    "    \"\"\"VAE-based: encode user profile to latent space, find nearest recipes.\"\"\"\n",
    "    exclude = set(exclude or [])\n",
    "\n",
    "    if user_id not in user_profiles:\n",
    "        return []\n",
    "\n",
    "    pref = user_profiles[user_id]\n",
    "\n",
    "    # Extract 12D features from the 32D preference vector (first 12 dims)\n",
    "    # In production, we use extract_features(); here we approximate\n",
    "    feat_12d = np.zeros(12)\n",
    "    feat_12d[:min(12, len(pref))] = pref[:12]\n",
    "\n",
    "    # Encode to latent space\n",
    "    normalized = (feat_12d - vae_w[\"feature_means\"]) / (vae_w[\"feature_stds\"] + 1e-8)\n",
    "    user_latent = normalized @ vae_w[\"encoder_mu_w\"] + vae_w[\"encoder_mu_b\"]\n",
    "    user_latent_norm = np.linalg.norm(user_latent)\n",
    "\n",
    "    scored = []\n",
    "    for rid, emb in recipe_features.items():\n",
    "        if rid in exclude:\n",
    "            continue\n",
    "        feat = np.zeros(12)\n",
    "        feat[:min(12, len(emb))] = emb[:12]\n",
    "        norm_feat = (feat - vae_w[\"feature_means\"]) / (vae_w[\"feature_stds\"] + 1e-8)\n",
    "        recipe_latent = norm_feat @ vae_w[\"encoder_mu_w\"] + vae_w[\"encoder_mu_b\"]\n",
    "        r_norm = np.linalg.norm(recipe_latent)\n",
    "\n",
    "        if user_latent_norm > 0 and r_norm > 0:\n",
    "            sim = float(np.dot(user_latent, recipe_latent) / (user_latent_norm * r_norm))\n",
    "        else:\n",
    "            sim = 0.0\n",
    "        scored.append((rid, sim))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [rid for rid, _ in scored[:top_n]]\n",
    "\n",
    "\n",
    "def rnn_recommend(user_id, train_data, recipe_features, rnn_w, top_n=10, exclude=None):\n",
    "    \"\"\"RNN-based: process user's meal history through GRU and predict next meal.\"\"\"\n",
    "    exclude = set(exclude or [])\n",
    "\n",
    "    # Get user's interactions in chronological order\n",
    "    user_meals = [(rid, rating) for uid, rid, rating in train_data if uid == user_id]\n",
    "    if len(user_meals) < 3:\n",
    "        return []\n",
    "\n",
    "    # Build sequence (use last 20 meals)\n",
    "    recent_meals = user_meals[-20:]\n",
    "    sequence = []\n",
    "    base_time = pd.Timestamp(\"2023-06-15 12:00:00\")\n",
    "\n",
    "    for i, (rid, rating) in enumerate(recent_meals):\n",
    "        emb = recipe_features.get(rid, np.zeros(32))\n",
    "        ts = base_time + pd.Timedelta(hours=i * 6)\n",
    "        hour = ts.hour\n",
    "        if 5 <= hour < 11:\n",
    "            mt = \"breakfast\"\n",
    "        elif 11 <= hour < 15:\n",
    "            mt = \"lunch\"\n",
    "        elif 15 <= hour < 21:\n",
    "            mt = \"dinner\"\n",
    "        else:\n",
    "            mt = \"snack\"\n",
    "        time_feat = _encode_time_features(ts, mt)\n",
    "        x = np.concatenate([emb, time_feat])\n",
    "        sequence.append(x)\n",
    "\n",
    "    # GRU forward pass\n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))\n",
    "\n",
    "    h = np.zeros(64, dtype=np.float64)\n",
    "    for x in sequence:\n",
    "        x = x.astype(np.float64)\n",
    "        z = sigmoid(x @ rnn_w[\"Wz\"] + h @ rnn_w[\"Uz\"] + rnn_w[\"bz\"])\n",
    "        r = sigmoid(x @ rnn_w[\"Wr\"] + h @ rnn_w[\"Ur\"] + rnn_w[\"br\"])\n",
    "        h_cand = np.tanh(x @ rnn_w[\"Wh\"] + (r * h) @ rnn_w[\"Uh\"] + rnn_w[\"bh\"])\n",
    "        h = (1 - z) * h + z * h_cand\n",
    "\n",
    "    predicted_emb = h @ rnn_w[\"Wo\"] + rnn_w[\"bo\"]\n",
    "    pred_norm = np.linalg.norm(predicted_emb)\n",
    "\n",
    "    # Score recipes by cosine similarity to prediction\n",
    "    scored = []\n",
    "    user_recipe_ids = {rid for rid, _ in user_meals}\n",
    "    for rid, emb in recipe_features.items():\n",
    "        if rid in exclude or rid in user_recipe_ids:\n",
    "            continue\n",
    "        emb_norm = np.linalg.norm(emb)\n",
    "        if pred_norm > 0 and emb_norm > 0:\n",
    "            sim = float(np.dot(predicted_emb, emb) / (pred_norm * emb_norm))\n",
    "        else:\n",
    "            sim = 0.0\n",
    "        scored.append((rid, sim))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [rid for rid, _ in scored[:top_n]]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Evaluate each model\n",
    "# ---------------------------------------------------------------------------\n",
    "TOP_N = 10\n",
    "test_users = list(test_data.keys())\n",
    "# Subsample if too many users (for speed)\n",
    "MAX_EVAL_USERS = 500\n",
    "if len(test_users) > MAX_EVAL_USERS:\n",
    "    rng_eval = np.random.default_rng(42)\n",
    "    test_users = list(rng_eval.choice(test_users, MAX_EVAL_USERS, replace=False))\n",
    "\n",
    "print(f\"Evaluating {len(test_users)} test users, top-{TOP_N} recommendations per model...\")\n",
    "\n",
    "# Pre-build user meal lookup for RNN/collaborative (avoid repeated scans)\n",
    "user_train_items = {}\n",
    "for uid, rid, rating in train_data:\n",
    "    if uid not in user_train_items:\n",
    "        user_train_items[uid] = set()\n",
    "    user_train_items[uid].add(rid)\n",
    "\n",
    "model_names = [\"content\", \"collaborative\", \"knowledge\", \"vae\", \"rnn\"]\n",
    "model_results = {name: {\"P@5\": [], \"P@10\": [], \"R@10\": [], \"NDCG@10\": [],\n",
    "                        \"MRR\": [], \"recs\": []} for name in model_names}\n",
    "\n",
    "for uid in tqdm(test_users, desc=\"Evaluating models\"):\n",
    "    relevant = test_data[uid]\n",
    "    exclude_items = user_train_items.get(uid, set())\n",
    "\n",
    "    # Content-based\n",
    "    recs = content_based_recommend(uid, user_profiles, recipe_features,\n",
    "                                   top_n=TOP_N, exclude=exclude_items)\n",
    "    model_results[\"content\"][\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "    model_results[\"content\"][\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "    model_results[\"content\"][\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "    model_results[\"content\"][\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "    model_results[\"content\"][\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "    model_results[\"content\"][\"recs\"].append(recs)\n",
    "\n",
    "    # Collaborative\n",
    "    recs = collaborative_recommend(uid, train_data, user_interaction_counts,\n",
    "                                    recipe_features, top_n=TOP_N, exclude=exclude_items)\n",
    "    model_results[\"collaborative\"][\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "    model_results[\"collaborative\"][\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "    model_results[\"collaborative\"][\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "    model_results[\"collaborative\"][\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "    model_results[\"collaborative\"][\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "    model_results[\"collaborative\"][\"recs\"].append(recs)\n",
    "\n",
    "    # Knowledge-based\n",
    "    recs = knowledge_based_recommend(uid, recipe_features,\n",
    "                                     top_n=TOP_N, exclude=exclude_items)\n",
    "    model_results[\"knowledge\"][\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "    model_results[\"knowledge\"][\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "    model_results[\"knowledge\"][\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "    model_results[\"knowledge\"][\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "    model_results[\"knowledge\"][\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "    model_results[\"knowledge\"][\"recs\"].append(recs)\n",
    "\n",
    "    # VAE\n",
    "    recs = vae_recommend(uid, user_profiles, recipe_features, vae_weights,\n",
    "                         top_n=TOP_N, exclude=exclude_items)\n",
    "    model_results[\"vae\"][\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "    model_results[\"vae\"][\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "    model_results[\"vae\"][\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "    model_results[\"vae\"][\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "    model_results[\"vae\"][\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "    model_results[\"vae\"][\"recs\"].append(recs)\n",
    "\n",
    "    # RNN\n",
    "    recs = rnn_recommend(uid, train_data, recipe_features, rnn_weights,\n",
    "                         top_n=TOP_N, exclude=exclude_items)\n",
    "    model_results[\"rnn\"][\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "    model_results[\"rnn\"][\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "    model_results[\"rnn\"][\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "    model_results[\"rnn\"][\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "    model_results[\"rnn\"][\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "    model_results[\"rnn\"][\"recs\"].append(recs)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Results table\n",
    "# ---------------------------------------------------------------------------\n",
    "metrics = [\"P@5\", \"P@10\", \"R@10\", \"NDCG@10\", \"MRR\"]\n",
    "results_table = []\n",
    "for name in model_names:\n",
    "    row = {\"Model\": name}\n",
    "    for m in metrics:\n",
    "        row[m] = np.mean(model_results[name][m])\n",
    "    # Coverage\n",
    "    row[\"Coverage\"] = coverage(model_results[name][\"recs\"], len(all_recipe_ids))\n",
    "    results_table.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results_table)\n",
    "results_df = results_df.set_index(\"Model\")\n",
    "\n",
    "print(\"\\nPer-Model Results\")\n",
    "print(\"=\" * 75)\n",
    "print(results_df.to_string(float_format=\"{:.4f}\".format))\n",
    "print()\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.15\n",
    "for i, name in enumerate(model_names):\n",
    "    values = [results_df.loc[name, m] for m in metrics]\n",
    "    ax.bar(x + i * width, values, width, label=name, color=PALETTE[i], alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Per-Model Recommendation Quality\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Blending Evaluation\n",
    "\n",
    "We replicate the production hybrid blending logic from `app/recommender/hybrid.py`:\n",
    "\n",
    "1. Each model generates candidate recommendations with raw scores.\n",
    "2. Per-model scores are **normalized to [0, 1]** (min-max within each model).\n",
    "3. Scores are **blended** using maturity-stage-dependent weights:\n",
    "\n",
    "| Stage | Knowledge | Content | Collaborative | VAE | RNN |\n",
    "|-------|-----------|---------|---------------|-----|-----|\n",
    "| Cold-start (<5) | 0.50 | 0.30 | 0.00 | 0.10 | 0.10 |\n",
    "| Early (5-19) | 0.25 | 0.30 | 0.15 | 0.15 | 0.15 |\n",
    "| Mature (20+) | 0.15 | 0.20 | 0.25 | 0.20 | 0.20 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Hybrid blending weights (from app/recommender/hybrid.py)\n",
    "# ---------------------------------------------------------------------------\n",
    "MODEL_WEIGHTS = {\n",
    "    \"cold_start\": {\n",
    "        \"knowledge\": 0.50,\n",
    "        \"content\": 0.30,\n",
    "        \"collaborative\": 0.00,\n",
    "        \"vae\": 0.10,\n",
    "        \"rnn\": 0.10,\n",
    "    },\n",
    "    \"early\": {\n",
    "        \"knowledge\": 0.25,\n",
    "        \"content\": 0.30,\n",
    "        \"collaborative\": 0.15,\n",
    "        \"vae\": 0.15,\n",
    "        \"rnn\": 0.15,\n",
    "    },\n",
    "    \"mature\": {\n",
    "        \"knowledge\": 0.15,\n",
    "        \"content\": 0.20,\n",
    "        \"collaborative\": 0.25,\n",
    "        \"vae\": 0.20,\n",
    "        \"rnn\": 0.20,\n",
    "    },\n",
    "}\n",
    "\n",
    "COLD_START_THRESHOLD = 5\n",
    "\n",
    "\n",
    "def get_maturity_stage(interaction_count):\n",
    "    if interaction_count < COLD_START_THRESHOLD:\n",
    "        return \"cold_start\"\n",
    "    elif interaction_count < 20:\n",
    "        return \"early\"\n",
    "    return \"mature\"\n",
    "\n",
    "\n",
    "def hybrid_recommend(user_id, user_profiles, train_data, recipe_features,\n",
    "                     vae_w, rnn_w, user_interaction_counts, top_n=10, exclude=None):\n",
    "    \"\"\"Hybrid recommendation matching the production blending logic.\"\"\"\n",
    "    exclude = set(exclude or [])\n",
    "    n_interactions = user_interaction_counts.get(user_id, 0)\n",
    "    maturity = get_maturity_stage(n_interactions)\n",
    "    weights = MODEL_WEIGHTS[maturity]\n",
    "\n",
    "    # Gather per-model scored candidates\n",
    "    all_model_scores = {}  # model_name -> [(recipe_id, score), ...]\n",
    "\n",
    "    # Content-based\n",
    "    if user_id in user_profiles:\n",
    "        pref = user_profiles[user_id]\n",
    "        pref_norm = np.linalg.norm(pref)\n",
    "        scored = []\n",
    "        for rid, emb in recipe_features.items():\n",
    "            if rid in exclude:\n",
    "                continue\n",
    "            emb_norm = np.linalg.norm(emb)\n",
    "            sim = float(np.dot(pref, emb) / (pref_norm * emb_norm + 1e-8))\n",
    "            scored.append((rid, sim))\n",
    "        all_model_scores[\"content\"] = scored\n",
    "    else:\n",
    "        all_model_scores[\"content\"] = []\n",
    "\n",
    "    # Knowledge-based\n",
    "    scored = []\n",
    "    for rid, emb in recipe_features.items():\n",
    "        if rid in exclude:\n",
    "            continue\n",
    "        cal = emb[0] * 1000\n",
    "        protein = emb[1] * 100\n",
    "        carbs = emb[2] * 200\n",
    "        fat = emb[3] * 100\n",
    "        cal_s = max(0, 1.0 - abs(cal - 500) / 500 * 0.5)\n",
    "        prot_s = max(0, 1.0 - abs(protein - 25) / 25 * 0.5)\n",
    "        carb_s = max(0, 1.0 - abs(carbs - 60) / 60 * 0.5)\n",
    "        fat_s = max(0, 1.0 - abs(fat - 20) / 20 * 0.5)\n",
    "        score = 0.3 * cal_s + 0.25 * prot_s + 0.25 * carb_s + 0.2 * fat_s\n",
    "        scored.append((rid, score))\n",
    "    all_model_scores[\"knowledge\"] = scored\n",
    "\n",
    "    # Collaborative\n",
    "    if maturity != \"cold_start\":\n",
    "        user_items = {}\n",
    "        for uid, rid, rating in train_data:\n",
    "            if uid not in user_items:\n",
    "                user_items[uid] = {}\n",
    "            user_items[uid][rid] = rating\n",
    "\n",
    "        if user_id in user_items:\n",
    "            target_items = user_items[user_id]\n",
    "            candidate_scores = {}\n",
    "\n",
    "            for other_uid, other_items in user_items.items():\n",
    "                if other_uid == user_id:\n",
    "                    continue\n",
    "                overlap = set(target_items.keys()) & set(other_items.keys())\n",
    "                if len(overlap) < 2:\n",
    "                    continue\n",
    "                v1 = np.array([target_items[r] for r in overlap])\n",
    "                v2 = np.array([other_items[r] for r in overlap])\n",
    "                n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "                if n1 > 0 and n2 > 0:\n",
    "                    sim = float(np.dot(v1, v2) / (n1 * n2))\n",
    "                    if sim > 0:\n",
    "                        for rid, rating in other_items.items():\n",
    "                            if rid not in target_items and rid not in exclude:\n",
    "                                candidate_scores[rid] = candidate_scores.get(rid, 0) + sim * rating\n",
    "\n",
    "            all_model_scores[\"collaborative\"] = list(candidate_scores.items())\n",
    "        else:\n",
    "            all_model_scores[\"collaborative\"] = []\n",
    "    else:\n",
    "        all_model_scores[\"collaborative\"] = []\n",
    "\n",
    "    # VAE\n",
    "    if user_id in user_profiles:\n",
    "        pref = user_profiles[user_id]\n",
    "        feat = np.zeros(12)\n",
    "        feat[:min(12, len(pref))] = pref[:12]\n",
    "        norm_feat = (feat - vae_w[\"feature_means\"]) / (vae_w[\"feature_stds\"] + 1e-8)\n",
    "        user_latent = norm_feat @ vae_w[\"encoder_mu_w\"] + vae_w[\"encoder_mu_b\"]\n",
    "        ul_norm = np.linalg.norm(user_latent)\n",
    "\n",
    "        scored = []\n",
    "        for rid, emb in recipe_features.items():\n",
    "            if rid in exclude:\n",
    "                continue\n",
    "            r_feat = np.zeros(12)\n",
    "            r_feat[:min(12, len(emb))] = emb[:12]\n",
    "            r_norm_feat = (r_feat - vae_w[\"feature_means\"]) / (vae_w[\"feature_stds\"] + 1e-8)\n",
    "            r_latent = r_norm_feat @ vae_w[\"encoder_mu_w\"] + vae_w[\"encoder_mu_b\"]\n",
    "            r_norm = np.linalg.norm(r_latent)\n",
    "            if ul_norm > 0 and r_norm > 0:\n",
    "                sim = float(np.dot(user_latent, r_latent) / (ul_norm * r_norm))\n",
    "            else:\n",
    "                sim = 0.0\n",
    "            scored.append((rid, sim))\n",
    "        all_model_scores[\"vae\"] = scored\n",
    "    else:\n",
    "        all_model_scores[\"vae\"] = []\n",
    "\n",
    "    # RNN\n",
    "    user_meals = [(rid, rating) for uid, rid, rating in train_data if uid == user_id]\n",
    "    if len(user_meals) >= 3:\n",
    "        recent = user_meals[-20:]\n",
    "        h = np.zeros(64, dtype=np.float64)\n",
    "        base_time = pd.Timestamp(\"2023-06-15 12:00:00\")\n",
    "        for i, (rid, _) in enumerate(recent):\n",
    "            emb = recipe_features.get(rid, np.zeros(32))\n",
    "            ts = base_time + pd.Timedelta(hours=i * 6)\n",
    "            hour = ts.hour\n",
    "            mt = \"breakfast\" if 5 <= hour < 11 else \"lunch\" if 11 <= hour < 15 else \"dinner\" if 15 <= hour < 21 else \"snack\"\n",
    "            tf = _encode_time_features(ts, mt)\n",
    "            x = np.concatenate([emb, tf]).astype(np.float64)\n",
    "\n",
    "            def sig(v):\n",
    "                return 1.0 / (1.0 + np.exp(-np.clip(v, -20, 20)))\n",
    "\n",
    "            z = sig(x @ rnn_w[\"Wz\"] + h @ rnn_w[\"Uz\"] + rnn_w[\"bz\"])\n",
    "            r = sig(x @ rnn_w[\"Wr\"] + h @ rnn_w[\"Ur\"] + rnn_w[\"br\"])\n",
    "            hc = np.tanh(x @ rnn_w[\"Wh\"] + (r * h) @ rnn_w[\"Uh\"] + rnn_w[\"bh\"])\n",
    "            h = (1 - z) * h + z * hc\n",
    "\n",
    "        pred = h @ rnn_w[\"Wo\"] + rnn_w[\"bo\"]\n",
    "        pred_norm = np.linalg.norm(pred)\n",
    "\n",
    "        user_rids = {rid for rid, _ in user_meals}\n",
    "        scored = []\n",
    "        for rid, emb in recipe_features.items():\n",
    "            if rid in exclude or rid in user_rids:\n",
    "                continue\n",
    "            en = np.linalg.norm(emb)\n",
    "            if pred_norm > 0 and en > 0:\n",
    "                sim = float(np.dot(pred, emb) / (pred_norm * en))\n",
    "            else:\n",
    "                sim = 0.0\n",
    "            scored.append((rid, sim))\n",
    "        all_model_scores[\"rnn\"] = scored\n",
    "    else:\n",
    "        all_model_scores[\"rnn\"] = []\n",
    "\n",
    "    # --- Normalize per-model scores to [0, 1] ---\n",
    "    for model_name, scored in all_model_scores.items():\n",
    "        if not scored:\n",
    "            continue\n",
    "        scores_only = [s for _, s in scored]\n",
    "        min_s = min(scores_only)\n",
    "        max_s = max(scores_only)\n",
    "        score_range = max_s - min_s\n",
    "        if score_range > 0:\n",
    "            all_model_scores[model_name] = [(rid, (s - min_s) / score_range) for rid, s in scored]\n",
    "        else:\n",
    "            all_model_scores[model_name] = [(rid, 1.0) for rid, s in scored]\n",
    "\n",
    "    # --- Blend with weights ---\n",
    "    combined = {}  # recipe_id -> blended score\n",
    "    for model_name, scored in all_model_scores.items():\n",
    "        w = weights.get(model_name, 0.0)\n",
    "        if w <= 0:\n",
    "            continue\n",
    "        for rid, score in scored:\n",
    "            combined[rid] = combined.get(rid, 0.0) + w * score\n",
    "\n",
    "    sorted_recs = sorted(combined.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [rid for rid, _ in sorted_recs[:top_n]]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Evaluate hybrid\n",
    "# ---------------------------------------------------------------------------\n",
    "hybrid_metrics = {\"P@5\": [], \"P@10\": [], \"R@10\": [], \"NDCG@10\": [], \"MRR\": [], \"recs\": []}\n",
    "\n",
    "for uid in tqdm(test_users, desc=\"Evaluating hybrid\"):\n",
    "    relevant = test_data[uid]\n",
    "    exclude_items = user_train_items.get(uid, set())\n",
    "\n",
    "    recs = hybrid_recommend(\n",
    "        uid, user_profiles, train_data, recipe_features,\n",
    "        vae_weights, rnn_weights, user_interaction_counts,\n",
    "        top_n=TOP_N, exclude=exclude_items\n",
    "    )\n",
    "\n",
    "    hybrid_metrics[\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "    hybrid_metrics[\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "    hybrid_metrics[\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "    hybrid_metrics[\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "    hybrid_metrics[\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "    hybrid_metrics[\"recs\"].append(recs)\n",
    "\n",
    "# Add hybrid to results\n",
    "hybrid_row = {\"Model\": \"HYBRID\"}\n",
    "for m in metrics:\n",
    "    hybrid_row[m] = np.mean(hybrid_metrics[m])\n",
    "hybrid_row[\"Coverage\"] = coverage(hybrid_metrics[\"recs\"], len(all_recipe_ids))\n",
    "\n",
    "full_results_df = pd.concat([\n",
    "    results_df,\n",
    "    pd.DataFrame([hybrid_row]).set_index(\"Model\")\n",
    "])\n",
    "\n",
    "print(\"\\nAll Model Results (including Hybrid)\")\n",
    "print(\"=\" * 80)\n",
    "print(full_results_df.to_string(float_format=\"{:.4f}\".format))\n",
    "\n",
    "# Highlight best per metric\n",
    "print(\"\\nBest model per metric:\")\n",
    "for m in metrics + [\"Coverage\"]:\n",
    "    best_model = full_results_df[m].idxmax()\n",
    "    best_val = full_results_df[m].max()\n",
    "    print(f\"  {m:<10} {best_model:<15} ({best_val:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Before vs After Comparison\n",
    "\n",
    "We compare the hybrid system's performance using **trained weights** vs **random weights**\n",
    "to quantify the impact of the training pipeline (notebooks 04 and 05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Generate random weights for comparison\n",
    "# ---------------------------------------------------------------------------\n",
    "rng_random = np.random.default_rng(42)\n",
    "\n",
    "random_vae_w = {}\n",
    "for key, shape in VAE_WEIGHT_SHAPES.items():\n",
    "    if \"b\" in key or \"mean\" in key:\n",
    "        random_vae_w[key] = np.zeros(shape)\n",
    "    elif \"std\" in key:\n",
    "        random_vae_w[key] = np.ones(shape)\n",
    "    else:\n",
    "        random_vae_w[key] = rng_random.standard_normal(shape) * 0.1\n",
    "\n",
    "random_rnn_w = {}\n",
    "for key, shape in RNN_WEIGHT_SHAPES.items():\n",
    "    if key.startswith(\"b\"):\n",
    "        random_rnn_w[key] = np.zeros(shape)\n",
    "    else:\n",
    "        random_rnn_w[key] = rng_random.standard_normal(shape) * 0.1\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Evaluate hybrid with random weights\n",
    "# ---------------------------------------------------------------------------\n",
    "random_metrics = {\"P@5\": [], \"P@10\": [], \"R@10\": [], \"NDCG@10\": [], \"MRR\": []}\n",
    "\n",
    "for uid in tqdm(test_users, desc=\"Evaluating (random weights)\"):\n",
    "    relevant = test_data[uid]\n",
    "    exclude_items = user_train_items.get(uid, set())\n",
    "\n",
    "    recs = hybrid_recommend(\n",
    "        uid, user_profiles, train_data, recipe_features,\n",
    "        random_vae_w, random_rnn_w, user_interaction_counts,\n",
    "        top_n=TOP_N, exclude=exclude_items\n",
    "    )\n",
    "\n",
    "    random_metrics[\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "    random_metrics[\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "    random_metrics[\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "    random_metrics[\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "    random_metrics[\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Comparison table\n",
    "# ---------------------------------------------------------------------------\n",
    "comparison = []\n",
    "for m in metrics:\n",
    "    random_val = np.mean(random_metrics[m])\n",
    "    trained_val = np.mean(hybrid_metrics[m])\n",
    "    improvement = trained_val - random_val\n",
    "    pct_change = (improvement / random_val * 100) if random_val > 0 else float(\"inf\")\n",
    "    comparison.append({\n",
    "        \"Metric\": m,\n",
    "        \"Random Weights\": random_val,\n",
    "        \"Trained Weights\": trained_val,\n",
    "        \"Improvement\": improvement,\n",
    "        \"% Change\": pct_change,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison).set_index(\"Metric\")\n",
    "print(\"Before vs After Training\")\n",
    "print(\"=\" * 75)\n",
    "print(comparison_df.to_string(float_format=\"{:.4f}\".format))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Bar chart\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "random_vals = [np.mean(random_metrics[m]) for m in metrics]\n",
    "trained_vals = [np.mean(hybrid_metrics[m]) for m in metrics]\n",
    "\n",
    "bars1 = ax.bar(x - width / 2, random_vals, width, label=\"Random Weights\",\n",
    "               color=SNACKTRACK_COLORS[\"neutral\"], alpha=0.7)\n",
    "bars2 = ax.bar(x + width / 2, trained_vals, width, label=\"Trained Weights\",\n",
    "               color=SNACKTRACK_COLORS[\"primary\"], alpha=0.85)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.002,\n",
    "            f\"{bar.get_height():.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.002,\n",
    "            f\"{bar.get_height():.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Hybrid Recommendation: Random vs Trained Weights\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Weight Sensitivity Analysis\n",
    "\n",
    "We explore how sensitive the hybrid's NDCG@10 is to changes in the model blending\n",
    "weights. For each model, we increase its weight by 0.1 and decrease by 0.1\n",
    "(re-normalizing the others), and measure the resulting NDCG@10.\n",
    "\n",
    "This helps determine:\n",
    "- Which models contribute most to overall quality\n",
    "- Whether the current weight configuration is near-optimal\n",
    "- How robust the system is to imprecise weight tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Weight sensitivity analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "# We test on the \"mature\" stage weights as it is the most balanced\n",
    "base_weights = MODEL_WEIGHTS[\"mature\"].copy()\n",
    "perturbation = 0.1\n",
    "sensitivity_model_names = [\"knowledge\", \"content\", \"collaborative\", \"vae\", \"rnn\"]\n",
    "\n",
    "# Subsample for speed\n",
    "MAX_SENSITIVITY_USERS = 200\n",
    "sensitivity_users = test_users[:MAX_SENSITIVITY_USERS]\n",
    "\n",
    "\n",
    "def evaluate_with_weights(custom_weights, users_subset):\n",
    "    \"\"\"Evaluate hybrid NDCG@10 with custom blending weights.\"\"\"\n",
    "    # Temporarily override MODEL_WEIGHTS for mature stage\n",
    "    temp_model_weights = {\n",
    "        \"cold_start\": MODEL_WEIGHTS[\"cold_start\"].copy(),\n",
    "        \"early\": MODEL_WEIGHTS[\"early\"].copy(),\n",
    "        \"mature\": custom_weights.copy(),\n",
    "    }\n",
    "\n",
    "    ndcg_scores = []\n",
    "    for uid in users_subset:\n",
    "        relevant = test_data[uid]\n",
    "        exclude_items = user_train_items.get(uid, set())\n",
    "        n_interactions = user_interaction_counts.get(uid, 0)\n",
    "        maturity = get_maturity_stage(n_interactions)\n",
    "        weights = temp_model_weights[maturity]\n",
    "\n",
    "        # Quick hybrid using pre-computed model scores would be ideal,\n",
    "        # but for correctness we re-run the full hybrid\n",
    "        # (using mature weights override)\n",
    "        recs = hybrid_recommend(\n",
    "            uid, user_profiles, train_data, recipe_features,\n",
    "            vae_weights, rnn_weights, user_interaction_counts,\n",
    "            top_n=TOP_N, exclude=exclude_items\n",
    "        )\n",
    "        ndcg_scores.append(ndcg_at_k(recs, relevant, 10))\n",
    "\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_ndcg = evaluate_with_weights(base_weights, sensitivity_users)\n",
    "\n",
    "# Perturb each model weight\n",
    "sensitivity_results = []\n",
    "\n",
    "for model_name in tqdm(sensitivity_model_names, desc=\"Sensitivity analysis\"):\n",
    "    for delta in [-perturbation, 0, perturbation]:\n",
    "        perturbed = base_weights.copy()\n",
    "        perturbed[model_name] = max(0, perturbed[model_name] + delta)\n",
    "\n",
    "        # Re-normalize remaining weights so they sum to ~1\n",
    "        total = sum(perturbed.values())\n",
    "        if total > 0:\n",
    "            perturbed = {k: v / total for k, v in perturbed.items()}\n",
    "\n",
    "        ndcg = evaluate_with_weights(perturbed, sensitivity_users)\n",
    "        sensitivity_results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Delta\": delta,\n",
    "            \"Weight\": perturbed[model_name],\n",
    "            \"NDCG@10\": ndcg,\n",
    "        })\n",
    "\n",
    "sens_df = pd.DataFrame(sensitivity_results)\n",
    "print(\"\\nWeight Sensitivity Analysis (NDCG@10)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pivot = sens_df.pivot(index=\"Model\", columns=\"Delta\", values=\"NDCG@10\")\n",
    "pivot.columns = [f\"delta={d:+.1f}\" for d in pivot.columns]\n",
    "print(pivot.to_string(float_format=\"{:.4f}\".format))\n",
    "print(f\"\\nBaseline NDCG@10 (current weights): {baseline_ndcg:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Heatmap\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "heatmap_data = sens_df.pivot(index=\"Model\", columns=\"Delta\", values=\"NDCG@10\")\n",
    "heatmap_data.columns = [f\"{d:+.1f}\" for d in heatmap_data.columns]\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cmap=\"YlGn\",\n",
    "            ax=ax, linewidths=1, linecolor=\"white\",\n",
    "            cbar_kws={\"label\": \"NDCG@10\"})\n",
    "ax.set_title(\"Weight Sensitivity: NDCG@10 by Model Weight Perturbation\")\n",
    "ax.set_xlabel(\"Weight Delta\")\n",
    "ax.set_ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cold-Start vs Mature Users\n",
    "\n",
    "We split test users into three segments based on their training-set interaction count:\n",
    "\n",
    "| Segment | Interaction Count | Characteristics |\n",
    "|---------|-------------------|----------------|\n",
    "| Cold-start | < 5 | Very little data; knowledge-based dominates |\n",
    "| Early | 5--19 | Some signal; balanced blend |\n",
    "| Mature | 20+ | Rich history; collaborative and RNN shine |\n",
    "\n",
    "This analysis reveals how well the adaptive blending weights serve each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Segment users\n",
    "# ---------------------------------------------------------------------------\n",
    "segments = {\n",
    "    \"Cold-start (<5)\": [],\n",
    "    \"Early (5-19)\": [],\n",
    "    \"Mature (20+)\": [],\n",
    "}\n",
    "\n",
    "for uid in test_users:\n",
    "    count = user_interaction_counts.get(uid, 0)\n",
    "    if count < 5:\n",
    "        segments[\"Cold-start (<5)\"].append(uid)\n",
    "    elif count < 20:\n",
    "        segments[\"Early (5-19)\"].append(uid)\n",
    "    else:\n",
    "        segments[\"Mature (20+)\"].append(uid)\n",
    "\n",
    "print(\"User Segments:\")\n",
    "for seg, users in segments.items():\n",
    "    print(f\"  {seg}: {len(users)} users\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Evaluate per segment\n",
    "# ---------------------------------------------------------------------------\n",
    "segment_results = []\n",
    "\n",
    "for seg_name, seg_users in segments.items():\n",
    "    if not seg_users:\n",
    "        segment_results.append({\n",
    "            \"Segment\": seg_name, \"Users\": 0,\n",
    "            \"P@5\": 0, \"P@10\": 0, \"R@10\": 0, \"NDCG@10\": 0, \"MRR\": 0,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    seg_metrics = {m: [] for m in metrics}\n",
    "\n",
    "    for uid in tqdm(seg_users, desc=f\"Eval {seg_name}\", leave=False):\n",
    "        relevant = test_data[uid]\n",
    "        exclude_items = user_train_items.get(uid, set())\n",
    "\n",
    "        recs = hybrid_recommend(\n",
    "            uid, user_profiles, train_data, recipe_features,\n",
    "            vae_weights, rnn_weights, user_interaction_counts,\n",
    "            top_n=TOP_N, exclude=exclude_items\n",
    "        )\n",
    "\n",
    "        seg_metrics[\"P@5\"].append(precision_at_k(recs, relevant, 5))\n",
    "        seg_metrics[\"P@10\"].append(precision_at_k(recs, relevant, 10))\n",
    "        seg_metrics[\"R@10\"].append(recall_at_k(recs, relevant, 10))\n",
    "        seg_metrics[\"NDCG@10\"].append(ndcg_at_k(recs, relevant, 10))\n",
    "        seg_metrics[\"MRR\"].append(mean_reciprocal_rank(recs, relevant))\n",
    "\n",
    "    row = {\"Segment\": seg_name, \"Users\": len(seg_users)}\n",
    "    for m in metrics:\n",
    "        row[m] = np.mean(seg_metrics[m])\n",
    "    segment_results.append(row)\n",
    "\n",
    "seg_df = pd.DataFrame(segment_results).set_index(\"Segment\")\n",
    "\n",
    "print(\"\\nHybrid Performance by User Segment\")\n",
    "print(\"=\" * 75)\n",
    "print(seg_df.to_string(float_format=\"{:.4f}\".format))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Bar chart\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "seg_names = [s for s in segments.keys() if len(segments[s]) > 0]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, seg_name in enumerate(seg_names):\n",
    "    values = [seg_df.loc[seg_name, m] for m in metrics]\n",
    "    ax.bar(x + i * width, values, width, label=seg_name,\n",
    "           color=PALETTE[i], alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x + width * (len(seg_names) - 1) / 2)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Hybrid Recommendation Quality by User Segment\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Final consolidated view of all evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Final summary\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=\" * 80)\n",
    "print(\"  SnackTrack Hybrid Recommendation System --- Evaluation Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. DATASET\")\n",
    "print(f\"   Total interactions:   {len(train_data):,} (train) + {sum(len(v) for v in test_data.values()):,} (test)\")\n",
    "print(f\"   Test users:           {len(test_data):,}\")\n",
    "print(f\"   Unique recipes:       {len(all_recipe_ids):,}\")\n",
    "print(f\"   Evaluation users:     {len(test_users)}\")\n",
    "\n",
    "print(\"\\n2. WEIGHT STATUS\")\n",
    "print(f\"   VAE: {'TRAINED' if vae_trained else 'RANDOM (run notebook 04 to train)'}\")\n",
    "print(f\"   RNN: {'TRAINED' if rnn_trained else 'RANDOM (run notebook 05 to train)'}\")\n",
    "\n",
    "print(\"\\n3. PER-MODEL RESULTS\")\n",
    "print(full_results_df.to_string(float_format=\"{:.4f}\".format))\n",
    "\n",
    "print(\"\\n4. BEST MODEL PER METRIC\")\n",
    "for m in metrics + [\"Coverage\"]:\n",
    "    best_model = full_results_df[m].idxmax()\n",
    "    best_val = full_results_df[m].max()\n",
    "    print(f\"   {m:<10}  {best_model:<15}  {best_val:.4f}\")\n",
    "\n",
    "print(\"\\n5. BEFORE vs AFTER TRAINING\")\n",
    "print(comparison_df.to_string(float_format=\"{:.4f}\".format))\n",
    "\n",
    "print(\"\\n6. PERFORMANCE BY USER SEGMENT\")\n",
    "print(seg_df.to_string(float_format=\"{:.4f}\".format))\n",
    "\n",
    "print(\"\\n7. KEY TAKEAWAYS\")\n",
    "hybrid_ndcg = np.mean(hybrid_metrics[\"NDCG@10\"])\n",
    "best_single_ndcg = results_df[\"NDCG@10\"].max()\n",
    "best_single_model = results_df[\"NDCG@10\"].idxmax()\n",
    "random_ndcg = np.mean(random_metrics[\"NDCG@10\"])\n",
    "\n",
    "print(f\"   - Hybrid NDCG@10:           {hybrid_ndcg:.4f}\")\n",
    "print(f\"   - Best single model:        {best_single_model} ({best_single_ndcg:.4f})\")\n",
    "if hybrid_ndcg > best_single_ndcg:\n",
    "    lift = (hybrid_ndcg - best_single_ndcg) / best_single_ndcg * 100\n",
    "    print(f\"   - Hybrid vs best single:    +{lift:.1f}% improvement\")\n",
    "else:\n",
    "    print(f\"   - Hybrid vs best single:    {best_single_model} is stronger alone\")\n",
    "\n",
    "if random_ndcg > 0:\n",
    "    training_lift = (hybrid_ndcg - random_ndcg) / random_ndcg * 100\n",
    "    print(f\"   - Trained vs random:        +{training_lift:.1f}% improvement\")\n",
    "\n",
    "print(f\"   - Coverage:                 {hybrid_row['Coverage']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  Evaluation complete.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}